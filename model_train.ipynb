{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fcfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "Loaded 53 labels, 26 entity types\n",
      "Random seed set\n",
      "All functions defined\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "sys.path.append('./model')\n",
    "from label_config import label2id, id2label, ENTITY_LABELS\n",
    "print(f\"Loaded {len(label2id)} labels, {len(ENTITY_LABELS)} entity types\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "set_seed(42)\n",
    "print(\"Random seed set\")\n",
    "\n",
    "\n",
    "class FinancialNERDataset(Dataset):\n",
    "    def __init__(self, jsonl_file, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.data.append(json.loads(line.strip()))\n",
    "        print(f\"Loaded {len(self.data)} examples from {jsonl_file.name}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        labels = item['ner_tag_ids']\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        padded_labels = labels + [-100] * (self.max_length - len(labels))\n",
    "        padded_labels = padded_labels[:self.max_length]\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(padded_labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        true_seq = []\n",
    "        pred_seq_labels = []\n",
    "        for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "            if label_id != -100:\n",
    "                true_seq.append(id2label[label_id])\n",
    "                pred_seq_labels.append(id2label[pred_id])\n",
    "        if true_seq:\n",
    "            true_labels.append(true_seq)\n",
    "            pred_labels.append(pred_seq_labels)\n",
    "    return {\n",
    "        'precision': precision_score(true_labels, pred_labels),\n",
    "        'recall': recall_score(true_labels, pred_labels),\n",
    "        'f1': f1_score(true_labels, pred_labels)\n",
    "    }\n",
    "\n",
    "print(\"All functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a88fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING MODEL AND DATA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([53]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([53, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: ProsusAI/finbert\n",
      "Loaded 19277 examples from train.jsonl\n",
      "Loaded 4060 examples from val.jsonl\n",
      "Loaded 3560 examples from test.jsonl\n",
      "\n",
      "Datasets loaded\n",
      "  Train: 19277\n",
      "  Val: 4060\n",
      "  Test: 3560\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path.cwd().parent\n",
    "data_dir = base_dir / 'data' / 'merged'\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = base_dir / 'model_outputs' / f'finbert_ner_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"LOADING MODEL AND DATA\")\n",
    "\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "\n",
    "train_dataset = FinancialNERDataset(data_dir / 'train.jsonl', tokenizer)\n",
    "val_dataset = FinancialNERDataset(data_dir / 'val.jsonl', tokenizer)\n",
    "test_dataset = FinancialNERDataset(data_dir / 'test.jsonl', tokenizer)\n",
    "\n",
    "print(f\"\\nDatasets loaded\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "batch_size = 10\n",
    "gradient_accumulation = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fadc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Hyperparameters:\n",
      "  Epochs:                    5\n",
      "  Batch Size:                10\n",
      "  Gradient Accumulation:     2\n",
      "  Effective Batch Size:      20\n",
      "  Learning Rate:             3e-05\n",
      "  Weight Decay:              0.01\n",
      "  Warmup Ratio:              0.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining Hyperparameters:\")\n",
    "print(f\"  Epochs:                    {num_epochs}\")\n",
    "print(f\"  Batch Size:                {batch_size}\")\n",
    "print(f\"  Gradient Accumulation:     {gradient_accumulation}\")\n",
    "print(f\"  Effective Batch Size:      {batch_size * gradient_accumulation}\")\n",
    "print(f\"  Learning Rate:             {learning_rate}\")\n",
    "print(f\"  Weight Decay:              0.01\")\n",
    "print(f\"  Warmup Ratio:              0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7807b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "Training for 5 epochs (4820 total steps)\n",
      "\n",
      "Epoch 1/5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Step   10/4820 (  0.2%) │ Loss: 3.9770 │ LR: 5.60e-07\n",
      "{'loss': 3.977, 'grad_norm': 7.581172466278076, 'learning_rate': 5.601659751037344e-07, 'epoch': 0.01037344398340249}\n",
      "Step   20/4820 (  0.4%) │ Loss: 3.9142 │ LR: 1.18e-06\n",
      "{'loss': 3.9142, 'grad_norm': 7.770468235015869, 'learning_rate': 1.1825726141078837e-06, 'epoch': 0.02074688796680498}\n",
      "Step   30/4820 (  0.6%) │ Loss: 3.7855 │ LR: 1.80e-06\n",
      "{'loss': 3.7855, 'grad_norm': 7.890115261077881, 'learning_rate': 1.8049792531120333e-06, 'epoch': 0.03112033195020747}\n",
      "Step   40/4820 (  0.8%) │ Loss: 3.5739 │ LR: 2.43e-06\n",
      "{'loss': 3.5739, 'grad_norm': 7.936939239501953, 'learning_rate': 2.4273858921161828e-06, 'epoch': 0.04149377593360996}\n",
      "Step   50/4820 (  1.0%) │ Loss: 3.2876 │ LR: 3.05e-06\n",
      "{'loss': 3.2876, 'grad_norm': 7.796020984649658, 'learning_rate': 3.049792531120332e-06, 'epoch': 0.05186721991701245}\n",
      "Step   60/4820 (  1.2%) │ Loss: 2.9592 │ LR: 3.67e-06\n",
      "{'loss': 2.9592, 'grad_norm': 7.6394805908203125, 'learning_rate': 3.672199170124481e-06, 'epoch': 0.06224066390041494}\n",
      "Step   70/4820 (  1.5%) │ Loss: 2.5078 │ LR: 4.29e-06\n",
      "{'loss': 2.5078, 'grad_norm': 5.09823751449585, 'learning_rate': 4.29460580912863e-06, 'epoch': 0.07261410788381743}\n",
      "Step   80/4820 (  1.7%) │ Loss: 2.1953 │ LR: 4.92e-06\n",
      "{'loss': 2.1953, 'grad_norm': 2.490577220916748, 'learning_rate': 4.91701244813278e-06, 'epoch': 0.08298755186721991}\n",
      "Step   90/4820 (  1.9%) │ Loss: 1.9963 │ LR: 5.54e-06\n",
      "{'loss': 1.9963, 'grad_norm': 1.9339959621429443, 'learning_rate': 5.53941908713693e-06, 'epoch': 0.09336099585062241}\n",
      "Step  100/4820 (  2.1%) │ Loss: 1.8515 │ LR: 6.16e-06\n",
      "{'loss': 1.8515, 'grad_norm': 1.7310763597488403, 'learning_rate': 6.161825726141079e-06, 'epoch': 0.1037344398340249}\n",
      "Step  110/4820 (  2.3%) │ Loss: 1.6989 │ LR: 6.78e-06\n",
      "{'loss': 1.6989, 'grad_norm': 1.470611810684204, 'learning_rate': 6.784232365145229e-06, 'epoch': 0.11410788381742738}\n",
      "Step  120/4820 (  2.5%) │ Loss: 1.5982 │ LR: 7.41e-06\n",
      "{'loss': 1.5982, 'grad_norm': 1.456976056098938, 'learning_rate': 7.4066390041493785e-06, 'epoch': 0.12448132780082988}\n",
      "Step  130/4820 (  2.7%) │ Loss: 1.4493 │ LR: 8.03e-06\n",
      "{'loss': 1.4493, 'grad_norm': 1.8575900793075562, 'learning_rate': 8.029045643153527e-06, 'epoch': 0.13485477178423236}\n",
      "Step  140/4820 (  2.9%) │ Loss: 1.3493 │ LR: 8.65e-06\n",
      "{'loss': 1.3493, 'grad_norm': 1.7968287467956543, 'learning_rate': 8.651452282157676e-06, 'epoch': 0.14522821576763487}\n",
      "Step  150/4820 (  3.1%) │ Loss: 1.2338 │ LR: 9.27e-06\n",
      "{'loss': 1.2338, 'grad_norm': 1.9740289449691772, 'learning_rate': 9.273858921161826e-06, 'epoch': 0.15560165975103735}\n",
      "Step  160/4820 (  3.3%) │ Loss: 1.1757 │ LR: 9.90e-06\n",
      "{'loss': 1.1757, 'grad_norm': 1.7883374691009521, 'learning_rate': 9.896265560165974e-06, 'epoch': 0.16597510373443983}\n",
      "Step  170/4820 (  3.5%) │ Loss: 1.1655 │ LR: 1.05e-05\n",
      "{'loss': 1.1655, 'grad_norm': 1.5382628440856934, 'learning_rate': 1.0518672199170124e-05, 'epoch': 0.17634854771784234}\n",
      "Step  180/4820 (  3.7%) │ Loss: 1.0373 │ LR: 1.11e-05\n",
      "{'loss': 1.0373, 'grad_norm': 1.9296631813049316, 'learning_rate': 1.1141078838174274e-05, 'epoch': 0.18672199170124482}\n",
      "Step  190/4820 (  3.9%) │ Loss: 0.9906 │ LR: 1.18e-05\n",
      "{'loss': 0.9906, 'grad_norm': 1.538044810295105, 'learning_rate': 1.1763485477178424e-05, 'epoch': 0.1970954356846473}\n",
      "Step  200/4820 (  4.1%) │ Loss: 0.9016 │ LR: 1.24e-05\n",
      "{'loss': 0.9016, 'grad_norm': 1.345339298248291, 'learning_rate': 1.2385892116182572e-05, 'epoch': 0.2074688796680498}\n",
      "Step  210/4820 (  4.4%) │ Loss: 0.9044 │ LR: 1.30e-05\n",
      "{'loss': 0.9044, 'grad_norm': 1.7521488666534424, 'learning_rate': 1.3008298755186722e-05, 'epoch': 0.21784232365145229}\n",
      "Step  220/4820 (  4.6%) │ Loss: 0.8815 │ LR: 1.36e-05\n",
      "{'loss': 0.8815, 'grad_norm': 1.528127670288086, 'learning_rate': 1.3630705394190872e-05, 'epoch': 0.22821576763485477}\n",
      "Step  230/4820 (  4.8%) │ Loss: 0.7850 │ LR: 1.43e-05\n",
      "{'loss': 0.785, 'grad_norm': 2.010052442550659, 'learning_rate': 1.425311203319502e-05, 'epoch': 0.23858921161825727}\n",
      "Step  240/4820 (  5.0%) │ Loss: 0.8561 │ LR: 1.49e-05\n",
      "{'loss': 0.8561, 'grad_norm': 1.488413691520691, 'learning_rate': 1.4875518672199171e-05, 'epoch': 0.24896265560165975}\n",
      "Step  250/4820 (  5.2%) │ Loss: 0.8301 │ LR: 1.55e-05\n",
      "{'loss': 0.8301, 'grad_norm': 1.7170966863632202, 'learning_rate': 1.549792531120332e-05, 'epoch': 0.25933609958506226}\n",
      "Step  260/4820 (  5.4%) │ Loss: 0.8208 │ LR: 1.61e-05\n",
      "{'loss': 0.8208, 'grad_norm': 1.8750369548797607, 'learning_rate': 1.612033195020747e-05, 'epoch': 0.2697095435684647}\n",
      "Step  270/4820 (  5.6%) │ Loss: 0.7229 │ LR: 1.67e-05\n",
      "{'loss': 0.7229, 'grad_norm': 1.6169778108596802, 'learning_rate': 1.6742738589211618e-05, 'epoch': 0.2800829875518672}\n",
      "Step  280/4820 (  5.8%) │ Loss: 0.7499 │ LR: 1.74e-05\n",
      "{'loss': 0.7499, 'grad_norm': 2.094510078430176, 'learning_rate': 1.736514522821577e-05, 'epoch': 0.29045643153526973}\n",
      "Step  290/4820 (  6.0%) │ Loss: 0.6404 │ LR: 1.80e-05\n",
      "{'loss': 0.6404, 'grad_norm': 1.775930404663086, 'learning_rate': 1.7987551867219917e-05, 'epoch': 0.3008298755186722}\n",
      "Step  300/4820 (  6.2%) │ Loss: 0.6196 │ LR: 1.86e-05\n",
      "{'loss': 0.6196, 'grad_norm': 2.1126132011413574, 'learning_rate': 1.860995850622407e-05, 'epoch': 0.3112033195020747}\n",
      "Step  310/4820 (  6.4%) │ Loss: 0.6025 │ LR: 1.92e-05\n",
      "{'loss': 0.6025, 'grad_norm': 2.1901872158050537, 'learning_rate': 1.9232365145228217e-05, 'epoch': 0.3215767634854772}\n",
      "Step  320/4820 (  6.6%) │ Loss: 0.5339 │ LR: 1.99e-05\n",
      "{'loss': 0.5339, 'grad_norm': 2.078768730163574, 'learning_rate': 1.9854771784232365e-05, 'epoch': 0.33195020746887965}\n",
      "Step  330/4820 (  6.8%) │ Loss: 0.5698 │ LR: 2.05e-05\n",
      "{'loss': 0.5698, 'grad_norm': 3.066431760787964, 'learning_rate': 2.0477178423236517e-05, 'epoch': 0.34232365145228216}\n",
      "Step  340/4820 (  7.1%) │ Loss: 0.5345 │ LR: 2.11e-05\n",
      "{'loss': 0.5345, 'grad_norm': 2.4273781776428223, 'learning_rate': 2.1099585062240665e-05, 'epoch': 0.35269709543568467}\n",
      "Step  350/4820 (  7.3%) │ Loss: 0.5099 │ LR: 2.17e-05\n",
      "{'loss': 0.5099, 'grad_norm': 2.17407488822937, 'learning_rate': 2.1721991701244813e-05, 'epoch': 0.3630705394190871}\n",
      "Step  360/4820 (  7.5%) │ Loss: 0.4972 │ LR: 2.23e-05\n",
      "{'loss': 0.4972, 'grad_norm': 2.253082036972046, 'learning_rate': 2.2344398340248964e-05, 'epoch': 0.37344398340248963}\n",
      "Step  370/4820 (  7.7%) │ Loss: 0.4971 │ LR: 2.30e-05\n",
      "{'loss': 0.4971, 'grad_norm': 1.952807903289795, 'learning_rate': 2.2966804979253113e-05, 'epoch': 0.38381742738589214}\n",
      "Step  380/4820 (  7.9%) │ Loss: 0.4447 │ LR: 2.36e-05\n",
      "{'loss': 0.4447, 'grad_norm': 2.4240670204162598, 'learning_rate': 2.358921161825726e-05, 'epoch': 0.3941908713692946}\n",
      "Step  390/4820 (  8.1%) │ Loss: 0.4315 │ LR: 2.42e-05\n",
      "{'loss': 0.4315, 'grad_norm': 2.3948757648468018, 'learning_rate': 2.4211618257261412e-05, 'epoch': 0.4045643153526971}\n",
      "Step  400/4820 (  8.3%) │ Loss: 0.4168 │ LR: 2.48e-05\n",
      "{'loss': 0.4168, 'grad_norm': 1.9399293661117554, 'learning_rate': 2.483402489626556e-05, 'epoch': 0.4149377593360996}\n",
      "Step  410/4820 (  8.5%) │ Loss: 0.3883 │ LR: 2.55e-05\n",
      "{'loss': 0.3883, 'grad_norm': 3.6681289672851562, 'learning_rate': 2.545643153526971e-05, 'epoch': 0.42531120331950206}\n",
      "Step  420/4820 (  8.7%) │ Loss: 0.3649 │ LR: 2.61e-05\n",
      "{'loss': 0.3649, 'grad_norm': 2.527043581008911, 'learning_rate': 2.607883817427386e-05, 'epoch': 0.43568464730290457}\n",
      "Step  430/4820 (  8.9%) │ Loss: 0.3688 │ LR: 2.67e-05\n",
      "{'loss': 0.3688, 'grad_norm': 3.7830545902252197, 'learning_rate': 2.6701244813278008e-05, 'epoch': 0.4460580912863071}\n",
      "Step  440/4820 (  9.1%) │ Loss: 0.3438 │ LR: 2.73e-05\n",
      "{'loss': 0.3438, 'grad_norm': 2.588671922683716, 'learning_rate': 2.7323651452282156e-05, 'epoch': 0.45643153526970953}\n",
      "Step  450/4820 (  9.3%) │ Loss: 0.3372 │ LR: 2.79e-05\n",
      "{'loss': 0.3372, 'grad_norm': 1.6970281600952148, 'learning_rate': 2.7946058091286308e-05, 'epoch': 0.46680497925311204}\n",
      "Step  460/4820 (  9.5%) │ Loss: 0.3468 │ LR: 2.86e-05\n",
      "{'loss': 0.3468, 'grad_norm': 1.9392162561416626, 'learning_rate': 2.8568464730290456e-05, 'epoch': 0.47717842323651455}\n",
      "Step  470/4820 (  9.8%) │ Loss: 0.3341 │ LR: 2.92e-05\n",
      "{'loss': 0.3341, 'grad_norm': 1.9513975381851196, 'learning_rate': 2.9190871369294607e-05, 'epoch': 0.487551867219917}\n",
      "Step  480/4820 ( 10.0%) │ Loss: 0.3425 │ LR: 2.98e-05\n",
      "{'loss': 0.3425, 'grad_norm': 3.0196523666381836, 'learning_rate': 2.9813278008298756e-05, 'epoch': 0.4979253112033195}\n",
      "Step  490/4820 ( 10.2%) │ Loss: 0.3200 │ LR: 3.00e-05\n",
      "{'loss': 0.32, 'grad_norm': 2.6559393405914307, 'learning_rate': 2.9951590594744122e-05, 'epoch': 0.508298755186722}\n",
      "Step  500/4820 ( 10.4%) │ Loss: 0.3033 │ LR: 2.99e-05\n",
      "{'loss': 0.3033, 'grad_norm': 2.6455798149108887, 'learning_rate': 2.988243430152144e-05, 'epoch': 0.5186721991701245}\n",
      "Step  510/4820 ( 10.6%) │ Loss: 0.3333 │ LR: 2.98e-05\n",
      "{'loss': 0.3333, 'grad_norm': 3.5606448650360107, 'learning_rate': 2.9813278008298756e-05, 'epoch': 0.529045643153527}\n",
      "Step  520/4820 ( 10.8%) │ Loss: 0.2624 │ LR: 2.97e-05\n",
      "{'loss': 0.2624, 'grad_norm': 1.746882677078247, 'learning_rate': 2.9744121715076073e-05, 'epoch': 0.5394190871369294}\n",
      "Step  530/4820 ( 11.0%) │ Loss: 0.2399 │ LR: 2.97e-05\n",
      "{'loss': 0.2399, 'grad_norm': 3.0080833435058594, 'learning_rate': 2.967496542185339e-05, 'epoch': 0.549792531120332}\n",
      "Step  540/4820 ( 11.2%) │ Loss: 0.2435 │ LR: 2.96e-05\n",
      "{'loss': 0.2435, 'grad_norm': 1.4214915037155151, 'learning_rate': 2.9605809128630706e-05, 'epoch': 0.5601659751037344}\n",
      "Step  550/4820 ( 11.4%) │ Loss: 0.2248 │ LR: 2.95e-05\n",
      "{'loss': 0.2248, 'grad_norm': 3.6229803562164307, 'learning_rate': 2.9536652835408023e-05, 'epoch': 0.5705394190871369}\n",
      "Step  560/4820 ( 11.6%) │ Loss: 0.2252 │ LR: 2.95e-05\n",
      "{'loss': 0.2252, 'grad_norm': 1.9586087465286255, 'learning_rate': 2.9467496542185337e-05, 'epoch': 0.5809128630705395}\n",
      "Step  570/4820 ( 11.8%) │ Loss: 0.2139 │ LR: 2.94e-05\n",
      "{'loss': 0.2139, 'grad_norm': 2.982057571411133, 'learning_rate': 2.9398340248962653e-05, 'epoch': 0.5912863070539419}\n",
      "Step  580/4820 ( 12.0%) │ Loss: 0.2352 │ LR: 2.93e-05\n",
      "{'loss': 0.2352, 'grad_norm': 2.416107177734375, 'learning_rate': 2.9329183955739974e-05, 'epoch': 0.6016597510373444}\n",
      "Step  590/4820 ( 12.2%) │ Loss: 0.2502 │ LR: 2.93e-05\n",
      "{'loss': 0.2502, 'grad_norm': 2.568537712097168, 'learning_rate': 2.926002766251729e-05, 'epoch': 0.6120331950207469}\n",
      "Step  600/4820 ( 12.4%) │ Loss: 0.2167 │ LR: 2.92e-05\n",
      "{'loss': 0.2167, 'grad_norm': 1.709269642829895, 'learning_rate': 2.9190871369294607e-05, 'epoch': 0.6224066390041494}\n",
      "Step  610/4820 ( 12.7%) │ Loss: 0.2542 │ LR: 2.91e-05\n",
      "{'loss': 0.2542, 'grad_norm': 2.4313666820526123, 'learning_rate': 2.9121715076071924e-05, 'epoch': 0.6327800829875518}\n",
      "Step  620/4820 ( 12.9%) │ Loss: 0.2377 │ LR: 2.91e-05\n",
      "{'loss': 0.2377, 'grad_norm': 2.7179551124572754, 'learning_rate': 2.905255878284924e-05, 'epoch': 0.6431535269709544}\n",
      "Step  630/4820 ( 13.1%) │ Loss: 0.2251 │ LR: 2.90e-05\n",
      "{'loss': 0.2251, 'grad_norm': 2.5017571449279785, 'learning_rate': 2.8983402489626558e-05, 'epoch': 0.6535269709543569}\n",
      "Step  640/4820 ( 13.3%) │ Loss: 0.2040 │ LR: 2.89e-05\n",
      "{'loss': 0.204, 'grad_norm': 2.489708185195923, 'learning_rate': 2.8914246196403875e-05, 'epoch': 0.6639004149377593}\n",
      "Step  650/4820 ( 13.5%) │ Loss: 0.2014 │ LR: 2.88e-05\n",
      "{'loss': 0.2014, 'grad_norm': 3.2263715267181396, 'learning_rate': 2.8845089903181192e-05, 'epoch': 0.6742738589211619}\n",
      "Step  660/4820 ( 13.7%) │ Loss: 0.1651 │ LR: 2.88e-05\n",
      "{'loss': 0.1651, 'grad_norm': 3.106750726699829, 'learning_rate': 2.877593360995851e-05, 'epoch': 0.6846473029045643}\n",
      "Step  670/4820 ( 13.9%) │ Loss: 0.1810 │ LR: 2.87e-05\n",
      "{'loss': 0.181, 'grad_norm': 2.286414384841919, 'learning_rate': 2.8706777316735826e-05, 'epoch': 0.6950207468879668}\n",
      "Step  680/4820 ( 14.1%) │ Loss: 0.2164 │ LR: 2.86e-05\n",
      "{'loss': 0.2164, 'grad_norm': 1.9895607233047485, 'learning_rate': 2.8637621023513142e-05, 'epoch': 0.7053941908713693}\n",
      "Step  690/4820 ( 14.3%) │ Loss: 0.1404 │ LR: 2.86e-05\n",
      "{'loss': 0.1404, 'grad_norm': 1.7147526741027832, 'learning_rate': 2.8568464730290456e-05, 'epoch': 0.7157676348547718}\n",
      "Step  700/4820 ( 14.5%) │ Loss: 0.1586 │ LR: 2.85e-05\n",
      "{'loss': 0.1586, 'grad_norm': 1.5166971683502197, 'learning_rate': 2.8499308437067773e-05, 'epoch': 0.7261410788381742}\n",
      "Step  710/4820 ( 14.7%) │ Loss: 0.2096 │ LR: 2.84e-05\n",
      "{'loss': 0.2096, 'grad_norm': 1.5285022258758545, 'learning_rate': 2.843015214384509e-05, 'epoch': 0.7365145228215768}\n",
      "Step  720/4820 ( 14.9%) │ Loss: 0.1524 │ LR: 2.84e-05\n",
      "{'loss': 0.1524, 'grad_norm': 1.8431133031845093, 'learning_rate': 2.8360995850622407e-05, 'epoch': 0.7468879668049793}\n",
      "Step  730/4820 ( 15.1%) │ Loss: 0.2313 │ LR: 2.83e-05\n",
      "{'loss': 0.2313, 'grad_norm': 2.2950844764709473, 'learning_rate': 2.8291839557399723e-05, 'epoch': 0.7572614107883817}\n",
      "Step  740/4820 ( 15.4%) │ Loss: 0.1502 │ LR: 2.82e-05\n",
      "{'loss': 0.1502, 'grad_norm': 3.105217218399048, 'learning_rate': 2.822268326417704e-05, 'epoch': 0.7676348547717843}\n",
      "Step  750/4820 ( 15.6%) │ Loss: 0.1736 │ LR: 2.82e-05\n",
      "{'loss': 0.1736, 'grad_norm': 2.52355694770813, 'learning_rate': 2.8153526970954357e-05, 'epoch': 0.7780082987551867}\n",
      "Step  760/4820 ( 15.8%) │ Loss: 0.1921 │ LR: 2.81e-05\n",
      "{'loss': 0.1921, 'grad_norm': 1.7859525680541992, 'learning_rate': 2.8084370677731674e-05, 'epoch': 0.7883817427385892}\n",
      "Step  770/4820 ( 16.0%) │ Loss: 0.1571 │ LR: 2.80e-05\n",
      "{'loss': 0.1571, 'grad_norm': 1.9019722938537598, 'learning_rate': 2.801521438450899e-05, 'epoch': 0.7987551867219918}\n",
      "Step  780/4820 ( 16.2%) │ Loss: 0.1590 │ LR: 2.79e-05\n",
      "{'loss': 0.159, 'grad_norm': 2.008343458175659, 'learning_rate': 2.7946058091286308e-05, 'epoch': 0.8091286307053942}\n",
      "Step  790/4820 ( 16.4%) │ Loss: 0.1635 │ LR: 2.79e-05\n",
      "{'loss': 0.1635, 'grad_norm': 1.928658127784729, 'learning_rate': 2.7876901798063625e-05, 'epoch': 0.8195020746887967}\n",
      "Step  800/4820 ( 16.6%) │ Loss: 0.1768 │ LR: 2.78e-05\n",
      "{'loss': 0.1768, 'grad_norm': 1.730968713760376, 'learning_rate': 2.780774550484094e-05, 'epoch': 0.8298755186721992}\n",
      "Step  810/4820 ( 16.8%) │ Loss: 0.1366 │ LR: 2.77e-05\n",
      "{'loss': 0.1366, 'grad_norm': 1.810533881187439, 'learning_rate': 2.773858921161826e-05, 'epoch': 0.8402489626556017}\n",
      "Step  820/4820 ( 17.0%) │ Loss: 0.1650 │ LR: 2.77e-05\n",
      "{'loss': 0.165, 'grad_norm': 3.704561948776245, 'learning_rate': 2.7669432918395575e-05, 'epoch': 0.8506224066390041}\n",
      "Step  830/4820 ( 17.2%) │ Loss: 0.1545 │ LR: 2.76e-05\n",
      "{'loss': 0.1545, 'grad_norm': 1.7807605266571045, 'learning_rate': 2.760027662517289e-05, 'epoch': 0.8609958506224067}\n",
      "Step  840/4820 ( 17.4%) │ Loss: 0.1343 │ LR: 2.75e-05\n",
      "{'loss': 0.1343, 'grad_norm': 2.847783088684082, 'learning_rate': 2.7531120331950206e-05, 'epoch': 0.8713692946058091}\n",
      "Step  850/4820 ( 17.6%) │ Loss: 0.1935 │ LR: 2.75e-05\n",
      "{'loss': 0.1935, 'grad_norm': 1.6355271339416504, 'learning_rate': 2.7461964038727523e-05, 'epoch': 0.8817427385892116}\n",
      "Step  860/4820 ( 17.8%) │ Loss: 0.1576 │ LR: 2.74e-05\n",
      "{'loss': 0.1576, 'grad_norm': 3.072784900665283, 'learning_rate': 2.739280774550484e-05, 'epoch': 0.8921161825726142}\n",
      "Step  870/4820 ( 18.0%) │ Loss: 0.1666 │ LR: 2.73e-05\n",
      "{'loss': 0.1666, 'grad_norm': 1.4825282096862793, 'learning_rate': 2.7323651452282156e-05, 'epoch': 0.9024896265560166}\n",
      "Step  880/4820 ( 18.3%) │ Loss: 0.1084 │ LR: 2.73e-05\n",
      "{'loss': 0.1084, 'grad_norm': 1.1692780256271362, 'learning_rate': 2.7254495159059477e-05, 'epoch': 0.9128630705394191}\n",
      "Step  890/4820 ( 18.5%) │ Loss: 0.1114 │ LR: 2.72e-05\n",
      "{'loss': 0.1114, 'grad_norm': 2.006608724594116, 'learning_rate': 2.7185338865836793e-05, 'epoch': 0.9232365145228216}\n",
      "Step  900/4820 ( 18.7%) │ Loss: 0.1372 │ LR: 2.71e-05\n",
      "{'loss': 0.1372, 'grad_norm': 2.693402051925659, 'learning_rate': 2.711618257261411e-05, 'epoch': 0.9336099585062241}\n",
      "Step  910/4820 ( 18.9%) │ Loss: 0.0997 │ LR: 2.70e-05\n",
      "{'loss': 0.0997, 'grad_norm': 3.0609912872314453, 'learning_rate': 2.7047026279391427e-05, 'epoch': 0.9439834024896265}\n",
      "Step  920/4820 ( 19.1%) │ Loss: 0.0969 │ LR: 2.70e-05\n",
      "{'loss': 0.0969, 'grad_norm': 2.01226806640625, 'learning_rate': 2.6977869986168744e-05, 'epoch': 0.9543568464730291}\n",
      "Step  930/4820 ( 19.3%) │ Loss: 0.1228 │ LR: 2.69e-05\n",
      "{'loss': 0.1228, 'grad_norm': 2.274712324142456, 'learning_rate': 2.690871369294606e-05, 'epoch': 0.9647302904564315}\n",
      "Step  940/4820 ( 19.5%) │ Loss: 0.1174 │ LR: 2.68e-05\n",
      "{'loss': 0.1174, 'grad_norm': 2.5058982372283936, 'learning_rate': 2.6839557399723378e-05, 'epoch': 0.975103734439834}\n",
      "Step  950/4820 ( 19.7%) │ Loss: 0.1443 │ LR: 2.68e-05\n",
      "{'loss': 0.1443, 'grad_norm': 1.7357773780822754, 'learning_rate': 2.6770401106500695e-05, 'epoch': 0.9854771784232366}\n",
      "Step  960/4820 ( 19.9%) │ Loss: 0.1290 │ LR: 2.67e-05\n",
      "{'loss': 0.129, 'grad_norm': 2.2851762771606445, 'learning_rate': 2.6701244813278008e-05, 'epoch': 0.995850622406639}\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Epoch completed in 704.6s\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Validation │ F1: 0.9314 │ Precision: 0.9390 │ Recall: 0.9239\n",
      "{'eval_loss': 0.10031117498874664, 'eval_precision': 0.9389886405276658, 'eval_recall': 0.9239228411754101, 'eval_f1': 0.9313948205361199, 'eval_runtime': 390.9618, 'eval_samples_per_second': 10.385, 'eval_steps_per_second': 3.463, 'epoch': 1.0}\n",
      "\n",
      "Epoch 2/5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Step  970/4820 ( 20.1%) │ Loss: 0.1097 │ LR: 2.66e-05\n",
      "{'loss': 0.1097, 'grad_norm': 2.3414692878723145, 'learning_rate': 2.6632088520055325e-05, 'epoch': 1.0062240663900415}\n",
      "Step  980/4820 ( 20.3%) │ Loss: 0.1340 │ LR: 2.66e-05\n",
      "{'loss': 0.134, 'grad_norm': 2.275331497192383, 'learning_rate': 2.6562932226832642e-05, 'epoch': 1.016597510373444}\n",
      "Step  990/4820 ( 20.5%) │ Loss: 0.0953 │ LR: 2.65e-05\n",
      "{'loss': 0.0953, 'grad_norm': 2.711541175842285, 'learning_rate': 2.649377593360996e-05, 'epoch': 1.0269709543568464}\n",
      "Step 1000/4820 ( 20.7%) │ Loss: 0.1814 │ LR: 2.64e-05\n",
      "{'loss': 0.1814, 'grad_norm': 1.6917061805725098, 'learning_rate': 2.6424619640387276e-05, 'epoch': 1.037344398340249}\n",
      "Step 1010/4820 ( 21.0%) │ Loss: 0.0987 │ LR: 2.64e-05\n",
      "{'loss': 0.0987, 'grad_norm': 1.3624119758605957, 'learning_rate': 2.6355463347164593e-05, 'epoch': 1.0477178423236515}\n",
      "Step 1020/4820 ( 21.2%) │ Loss: 0.0968 │ LR: 2.63e-05\n",
      "{'loss': 0.0968, 'grad_norm': 3.0515096187591553, 'learning_rate': 2.628630705394191e-05, 'epoch': 1.058091286307054}\n",
      "Step 1030/4820 ( 21.4%) │ Loss: 0.1107 │ LR: 2.62e-05\n",
      "{'loss': 0.1107, 'grad_norm': 2.791498899459839, 'learning_rate': 2.6217150760719226e-05, 'epoch': 1.0684647302904564}\n",
      "Step 1040/4820 ( 21.6%) │ Loss: 0.0801 │ LR: 2.61e-05\n",
      "{'loss': 0.0801, 'grad_norm': 2.1441519260406494, 'learning_rate': 2.6147994467496543e-05, 'epoch': 1.0788381742738589}\n",
      "Step 1050/4820 ( 21.8%) │ Loss: 0.0854 │ LR: 2.61e-05\n",
      "{'loss': 0.0854, 'grad_norm': 1.4447195529937744, 'learning_rate': 2.607883817427386e-05, 'epoch': 1.0892116182572613}\n",
      "Step 1060/4820 ( 22.0%) │ Loss: 0.1160 │ LR: 2.60e-05\n",
      "{'loss': 0.116, 'grad_norm': 1.4207441806793213, 'learning_rate': 2.6009681881051177e-05, 'epoch': 1.099585062240664}\n",
      "Step 1070/4820 ( 22.2%) │ Loss: 0.1328 │ LR: 2.59e-05\n",
      "{'loss': 0.1328, 'grad_norm': 1.3096145391464233, 'learning_rate': 2.5940525587828494e-05, 'epoch': 1.1099585062240664}\n",
      "Step 1080/4820 ( 22.4%) │ Loss: 0.1087 │ LR: 2.59e-05\n",
      "{'loss': 0.1087, 'grad_norm': 1.490282654762268, 'learning_rate': 2.587136929460581e-05, 'epoch': 1.120331950207469}\n",
      "Step 1090/4820 ( 22.6%) │ Loss: 0.0656 │ LR: 2.58e-05\n",
      "{'loss': 0.0656, 'grad_norm': 1.9658557176589966, 'learning_rate': 2.5802213001383128e-05, 'epoch': 1.1307053941908713}\n",
      "Step 1100/4820 ( 22.8%) │ Loss: 0.1002 │ LR: 2.57e-05\n",
      "{'loss': 0.1002, 'grad_norm': 1.6449030637741089, 'learning_rate': 2.573305670816044e-05, 'epoch': 1.1410788381742738}\n",
      "Step 1110/4820 ( 23.0%) │ Loss: 0.0725 │ LR: 2.57e-05\n",
      "{'loss': 0.0725, 'grad_norm': 1.800387978553772, 'learning_rate': 2.5663900414937758e-05, 'epoch': 1.1514522821576763}\n",
      "Step 1120/4820 ( 23.2%) │ Loss: 0.0721 │ LR: 2.56e-05\n",
      "{'loss': 0.0721, 'grad_norm': 1.5000687837600708, 'learning_rate': 2.5594744121715075e-05, 'epoch': 1.161825726141079}\n",
      "Step 1130/4820 ( 23.4%) │ Loss: 0.0854 │ LR: 2.55e-05\n",
      "{'loss': 0.0854, 'grad_norm': 1.9117491245269775, 'learning_rate': 2.552558782849239e-05, 'epoch': 1.1721991701244814}\n",
      "Step 1140/4820 ( 23.7%) │ Loss: 0.0929 │ LR: 2.55e-05\n",
      "{'loss': 0.0929, 'grad_norm': 1.4053964614868164, 'learning_rate': 2.545643153526971e-05, 'epoch': 1.1825726141078838}\n",
      "Step 1150/4820 ( 23.9%) │ Loss: 0.1613 │ LR: 2.54e-05\n",
      "{'loss': 0.1613, 'grad_norm': 1.6843688488006592, 'learning_rate': 2.5387275242047025e-05, 'epoch': 1.1929460580912863}\n",
      "Step 1160/4820 ( 24.1%) │ Loss: 0.0870 │ LR: 2.53e-05\n",
      "{'loss': 0.087, 'grad_norm': 1.9401652812957764, 'learning_rate': 2.5318118948824342e-05, 'epoch': 1.2033195020746887}\n",
      "Step 1170/4820 ( 24.3%) │ Loss: 0.1137 │ LR: 2.52e-05\n",
      "{'loss': 0.1137, 'grad_norm': 1.673512578010559, 'learning_rate': 2.524896265560166e-05, 'epoch': 1.2136929460580912}\n",
      "Step 1180/4820 ( 24.5%) │ Loss: 0.1189 │ LR: 2.52e-05\n",
      "{'loss': 0.1189, 'grad_norm': 3.0267910957336426, 'learning_rate': 2.5179806362378976e-05, 'epoch': 1.2240663900414939}\n",
      "Step 1190/4820 ( 24.7%) │ Loss: 0.1037 │ LR: 2.51e-05\n",
      "{'loss': 0.1037, 'grad_norm': 1.2842731475830078, 'learning_rate': 2.5110650069156296e-05, 'epoch': 1.2344398340248963}\n",
      "Step 1200/4820 ( 24.9%) │ Loss: 0.1095 │ LR: 2.50e-05\n",
      "{'loss': 0.1095, 'grad_norm': 1.8448572158813477, 'learning_rate': 2.5041493775933613e-05, 'epoch': 1.2448132780082988}\n",
      "Step 1210/4820 ( 25.1%) │ Loss: 0.0799 │ LR: 2.50e-05\n",
      "{'loss': 0.0799, 'grad_norm': 2.055795907974243, 'learning_rate': 2.497233748271093e-05, 'epoch': 1.2551867219917012}\n",
      "Step 1220/4820 ( 25.3%) │ Loss: 0.1152 │ LR: 2.49e-05\n",
      "{'loss': 0.1152, 'grad_norm': 3.453871488571167, 'learning_rate': 2.4903181189488247e-05, 'epoch': 1.2655601659751037}\n",
      "Step 1230/4820 ( 25.5%) │ Loss: 0.0865 │ LR: 2.48e-05\n",
      "{'loss': 0.0865, 'grad_norm': 1.6067724227905273, 'learning_rate': 2.483402489626556e-05, 'epoch': 1.2759336099585061}\n",
      "Step 1240/4820 ( 25.7%) │ Loss: 0.0999 │ LR: 2.48e-05\n",
      "{'loss': 0.0999, 'grad_norm': 1.0898666381835938, 'learning_rate': 2.4764868603042877e-05, 'epoch': 1.2863070539419086}\n",
      "Step 1250/4820 ( 25.9%) │ Loss: 0.0908 │ LR: 2.47e-05\n",
      "{'loss': 0.0908, 'grad_norm': 2.2446792125701904, 'learning_rate': 2.4695712309820194e-05, 'epoch': 1.2966804979253113}\n",
      "Step 1260/4820 ( 26.1%) │ Loss: 0.0898 │ LR: 2.46e-05\n",
      "{'loss': 0.0898, 'grad_norm': 1.4994443655014038, 'learning_rate': 2.462655601659751e-05, 'epoch': 1.3070539419087137}\n",
      "Step 1270/4820 ( 26.3%) │ Loss: 0.0643 │ LR: 2.46e-05\n",
      "{'loss': 0.0643, 'grad_norm': 1.8222503662109375, 'learning_rate': 2.4557399723374828e-05, 'epoch': 1.3174273858921162}\n",
      "Step 1280/4820 ( 26.6%) │ Loss: 0.1189 │ LR: 2.45e-05\n",
      "{'loss': 0.1189, 'grad_norm': 2.1439707279205322, 'learning_rate': 2.4488243430152145e-05, 'epoch': 1.3278008298755186}\n",
      "Step 1290/4820 ( 26.8%) │ Loss: 0.0788 │ LR: 2.44e-05\n",
      "{'loss': 0.0788, 'grad_norm': 1.0858205556869507, 'learning_rate': 2.441908713692946e-05, 'epoch': 1.3381742738589213}\n",
      "Step 1300/4820 ( 27.0%) │ Loss: 0.0722 │ LR: 2.43e-05\n",
      "{'loss': 0.0722, 'grad_norm': 1.8389501571655273, 'learning_rate': 2.434993084370678e-05, 'epoch': 1.3485477178423237}\n",
      "Step 1310/4820 ( 27.2%) │ Loss: 0.0738 │ LR: 2.43e-05\n",
      "{'loss': 0.0738, 'grad_norm': 1.3496710062026978, 'learning_rate': 2.4280774550484095e-05, 'epoch': 1.3589211618257262}\n",
      "Step 1320/4820 ( 27.4%) │ Loss: 0.0582 │ LR: 2.42e-05\n",
      "{'loss': 0.0582, 'grad_norm': 1.2666468620300293, 'learning_rate': 2.4211618257261412e-05, 'epoch': 1.3692946058091287}\n",
      "Step 1330/4820 ( 27.6%) │ Loss: 0.0776 │ LR: 2.41e-05\n",
      "{'loss': 0.0776, 'grad_norm': 1.7948201894760132, 'learning_rate': 2.414246196403873e-05, 'epoch': 1.379668049792531}\n",
      "Step 1340/4820 ( 27.8%) │ Loss: 0.0687 │ LR: 2.41e-05\n",
      "{'loss': 0.0687, 'grad_norm': 3.1533122062683105, 'learning_rate': 2.4073305670816046e-05, 'epoch': 1.3900414937759336}\n",
      "Step 1350/4820 ( 28.0%) │ Loss: 0.0875 │ LR: 2.40e-05\n",
      "{'loss': 0.0875, 'grad_norm': 2.6408913135528564, 'learning_rate': 2.4004149377593363e-05, 'epoch': 1.400414937759336}\n",
      "Step 1360/4820 ( 28.2%) │ Loss: 0.0748 │ LR: 2.39e-05\n",
      "{'loss': 0.0748, 'grad_norm': 1.5367686748504639, 'learning_rate': 2.393499308437068e-05, 'epoch': 1.4107883817427385}\n",
      "Step 1370/4820 ( 28.4%) │ Loss: 0.0987 │ LR: 2.39e-05\n",
      "{'loss': 0.0987, 'grad_norm': 1.587069034576416, 'learning_rate': 2.3865836791147993e-05, 'epoch': 1.4211618257261411}\n",
      "Step 1380/4820 ( 28.6%) │ Loss: 0.0701 │ LR: 2.38e-05\n",
      "{'loss': 0.0701, 'grad_norm': 1.0389834642410278, 'learning_rate': 2.379668049792531e-05, 'epoch': 1.4315352697095436}\n",
      "Step 1390/4820 ( 28.8%) │ Loss: 0.0734 │ LR: 2.37e-05\n",
      "{'loss': 0.0734, 'grad_norm': 1.0053154230117798, 'learning_rate': 2.3727524204702627e-05, 'epoch': 1.441908713692946}\n",
      "Step 1400/4820 ( 29.0%) │ Loss: 0.0738 │ LR: 2.37e-05\n",
      "{'loss': 0.0738, 'grad_norm': 1.5002061128616333, 'learning_rate': 2.3658367911479944e-05, 'epoch': 1.4522821576763485}\n",
      "Step 1410/4820 ( 29.3%) │ Loss: 0.0918 │ LR: 2.36e-05\n",
      "{'loss': 0.0918, 'grad_norm': 1.2507435083389282, 'learning_rate': 2.358921161825726e-05, 'epoch': 1.4626556016597512}\n",
      "Step 1420/4820 ( 29.5%) │ Loss: 0.0913 │ LR: 2.35e-05\n",
      "{'loss': 0.0913, 'grad_norm': 2.010610342025757, 'learning_rate': 2.3520055325034578e-05, 'epoch': 1.4730290456431536}\n",
      "Step 1430/4820 ( 29.7%) │ Loss: 0.0891 │ LR: 2.35e-05\n",
      "{'loss': 0.0891, 'grad_norm': 1.4218330383300781, 'learning_rate': 2.3450899031811894e-05, 'epoch': 1.483402489626556}\n",
      "Step 1440/4820 ( 29.9%) │ Loss: 0.0820 │ LR: 2.34e-05\n",
      "{'loss': 0.082, 'grad_norm': 3.253082275390625, 'learning_rate': 2.338174273858921e-05, 'epoch': 1.4937759336099585}\n",
      "Step 1450/4820 ( 30.1%) │ Loss: 0.0499 │ LR: 2.33e-05\n",
      "{'loss': 0.0499, 'grad_norm': 2.0415446758270264, 'learning_rate': 2.3312586445366528e-05, 'epoch': 1.504149377593361}\n",
      "Step 1460/4820 ( 30.3%) │ Loss: 0.0797 │ LR: 2.32e-05\n",
      "{'loss': 0.0797, 'grad_norm': 1.7988876104354858, 'learning_rate': 2.3243430152143845e-05, 'epoch': 1.5145228215767634}\n",
      "Step 1470/4820 ( 30.5%) │ Loss: 0.0629 │ LR: 2.32e-05\n",
      "{'loss': 0.0629, 'grad_norm': 2.621333360671997, 'learning_rate': 2.3174273858921162e-05, 'epoch': 1.5248962655601659}\n",
      "Step 1480/4820 ( 30.7%) │ Loss: 0.0731 │ LR: 2.31e-05\n",
      "{'loss': 0.0731, 'grad_norm': 1.5304967164993286, 'learning_rate': 2.310511756569848e-05, 'epoch': 1.5352697095435683}\n",
      "Step 1490/4820 ( 30.9%) │ Loss: 0.0724 │ LR: 2.30e-05\n",
      "{'loss': 0.0724, 'grad_norm': 1.2477234601974487, 'learning_rate': 2.3035961272475796e-05, 'epoch': 1.5456431535269708}\n",
      "Step 1500/4820 ( 31.1%) │ Loss: 0.0936 │ LR: 2.30e-05\n",
      "{'loss': 0.0936, 'grad_norm': 2.2021450996398926, 'learning_rate': 2.2966804979253113e-05, 'epoch': 1.5560165975103735}\n",
      "Step 1510/4820 ( 31.3%) │ Loss: 0.0800 │ LR: 2.29e-05\n",
      "{'loss': 0.08, 'grad_norm': 1.415604591369629, 'learning_rate': 2.289764868603043e-05, 'epoch': 1.566390041493776}\n",
      "Step 1520/4820 ( 31.5%) │ Loss: 0.0910 │ LR: 2.28e-05\n",
      "{'loss': 0.091, 'grad_norm': 2.1308610439300537, 'learning_rate': 2.2828492392807746e-05, 'epoch': 1.5767634854771784}\n",
      "Step 1530/4820 ( 31.7%) │ Loss: 0.0701 │ LR: 2.28e-05\n",
      "{'loss': 0.0701, 'grad_norm': 2.3989529609680176, 'learning_rate': 2.2759336099585063e-05, 'epoch': 1.587136929460581}\n",
      "Step 1540/4820 ( 32.0%) │ Loss: 0.0873 │ LR: 2.27e-05\n",
      "{'loss': 0.0873, 'grad_norm': 2.0670723915100098, 'learning_rate': 2.269017980636238e-05, 'epoch': 1.5975103734439835}\n",
      "Step 1550/4820 ( 32.2%) │ Loss: 0.0802 │ LR: 2.26e-05\n",
      "{'loss': 0.0802, 'grad_norm': 1.8111116886138916, 'learning_rate': 2.2621023513139697e-05, 'epoch': 1.607883817427386}\n",
      "Step 1560/4820 ( 32.4%) │ Loss: 0.0688 │ LR: 2.26e-05\n",
      "{'loss': 0.0688, 'grad_norm': 1.3456858396530151, 'learning_rate': 2.2551867219917014e-05, 'epoch': 1.6182572614107884}\n",
      "Step 1570/4820 ( 32.6%) │ Loss: 0.0998 │ LR: 2.25e-05\n",
      "{'loss': 0.0998, 'grad_norm': 1.696797490119934, 'learning_rate': 2.248271092669433e-05, 'epoch': 1.6286307053941909}\n",
      "Step 1580/4820 ( 32.8%) │ Loss: 0.0591 │ LR: 2.24e-05\n",
      "{'loss': 0.0591, 'grad_norm': 1.7652026414871216, 'learning_rate': 2.2413554633471648e-05, 'epoch': 1.6390041493775933}\n",
      "Step 1590/4820 ( 33.0%) │ Loss: 0.0654 │ LR: 2.23e-05\n",
      "{'loss': 0.0654, 'grad_norm': 1.7319849729537964, 'learning_rate': 2.2344398340248964e-05, 'epoch': 1.6493775933609958}\n",
      "Step 1600/4820 ( 33.2%) │ Loss: 0.0557 │ LR: 2.23e-05\n",
      "{'loss': 0.0557, 'grad_norm': 4.519812107086182, 'learning_rate': 2.227524204702628e-05, 'epoch': 1.6597510373443982}\n",
      "Step 1610/4820 ( 33.4%) │ Loss: 0.0946 │ LR: 2.22e-05\n",
      "{'loss': 0.0946, 'grad_norm': 3.475475788116455, 'learning_rate': 2.2206085753803598e-05, 'epoch': 1.6701244813278007}\n",
      "Step 1620/4820 ( 33.6%) │ Loss: 0.0896 │ LR: 2.21e-05\n",
      "{'loss': 0.0896, 'grad_norm': 0.88429856300354, 'learning_rate': 2.2136929460580915e-05, 'epoch': 1.6804979253112033}\n",
      "Step 1630/4820 ( 33.8%) │ Loss: 0.0714 │ LR: 2.21e-05\n",
      "{'loss': 0.0714, 'grad_norm': 2.1086604595184326, 'learning_rate': 2.2067773167358232e-05, 'epoch': 1.6908713692946058}\n",
      "Step 1640/4820 ( 34.0%) │ Loss: 0.0710 │ LR: 2.20e-05\n",
      "{'loss': 0.071, 'grad_norm': 0.507246732711792, 'learning_rate': 2.1998616874135545e-05, 'epoch': 1.7012448132780082}\n",
      "Step 1650/4820 ( 34.2%) │ Loss: 0.0704 │ LR: 2.19e-05\n",
      "{'loss': 0.0704, 'grad_norm': 0.5668859481811523, 'learning_rate': 2.1929460580912862e-05, 'epoch': 1.711618257261411}\n",
      "Step 1660/4820 ( 34.4%) │ Loss: 0.0782 │ LR: 2.19e-05\n",
      "{'loss': 0.0782, 'grad_norm': 2.7041568756103516, 'learning_rate': 2.186030428769018e-05, 'epoch': 1.7219917012448134}\n",
      "Step 1670/4820 ( 34.6%) │ Loss: 0.0550 │ LR: 2.18e-05\n",
      "{'loss': 0.055, 'grad_norm': 1.8378453254699707, 'learning_rate': 2.1791147994467496e-05, 'epoch': 1.7323651452282158}\n",
      "Step 1680/4820 ( 34.9%) │ Loss: 0.0960 │ LR: 2.17e-05\n",
      "{'loss': 0.096, 'grad_norm': 4.57888650894165, 'learning_rate': 2.1721991701244813e-05, 'epoch': 1.7427385892116183}\n",
      "Step 1690/4820 ( 35.1%) │ Loss: 0.0570 │ LR: 2.17e-05\n",
      "{'loss': 0.057, 'grad_norm': 3.5269463062286377, 'learning_rate': 2.165283540802213e-05, 'epoch': 1.7531120331950207}\n",
      "Step 1700/4820 ( 35.3%) │ Loss: 0.0646 │ LR: 2.16e-05\n",
      "{'loss': 0.0646, 'grad_norm': 2.1046266555786133, 'learning_rate': 2.1583679114799447e-05, 'epoch': 1.7634854771784232}\n",
      "Step 1710/4820 ( 35.5%) │ Loss: 0.0664 │ LR: 2.15e-05\n",
      "{'loss': 0.0664, 'grad_norm': 0.6617361903190613, 'learning_rate': 2.1514522821576763e-05, 'epoch': 1.7738589211618256}\n",
      "Step 1720/4820 ( 35.7%) │ Loss: 0.0917 │ LR: 2.14e-05\n",
      "{'loss': 0.0917, 'grad_norm': 0.6901171803474426, 'learning_rate': 2.144536652835408e-05, 'epoch': 1.784232365145228}\n",
      "Step 1730/4820 ( 35.9%) │ Loss: 0.0624 │ LR: 2.14e-05\n",
      "{'loss': 0.0624, 'grad_norm': 1.3600512742996216, 'learning_rate': 2.1376210235131397e-05, 'epoch': 1.7946058091286305}\n",
      "Step 1740/4820 ( 36.1%) │ Loss: 0.0535 │ LR: 2.13e-05\n",
      "{'loss': 0.0535, 'grad_norm': 0.836432933807373, 'learning_rate': 2.1307053941908714e-05, 'epoch': 1.8049792531120332}\n",
      "Step 1750/4820 ( 36.3%) │ Loss: 0.0541 │ LR: 2.12e-05\n",
      "{'loss': 0.0541, 'grad_norm': 0.44957950711250305, 'learning_rate': 2.123789764868603e-05, 'epoch': 1.8153526970954357}\n",
      "Step 1760/4820 ( 36.5%) │ Loss: 0.0762 │ LR: 2.12e-05\n",
      "{'loss': 0.0762, 'grad_norm': 2.1718204021453857, 'learning_rate': 2.1168741355463348e-05, 'epoch': 1.8257261410788381}\n",
      "Step 1770/4820 ( 36.7%) │ Loss: 0.0602 │ LR: 2.11e-05\n",
      "{'loss': 0.0602, 'grad_norm': 1.1408395767211914, 'learning_rate': 2.1099585062240665e-05, 'epoch': 1.8360995850622408}\n",
      "Step 1780/4820 ( 36.9%) │ Loss: 0.0880 │ LR: 2.10e-05\n",
      "{'loss': 0.088, 'grad_norm': 4.39764404296875, 'learning_rate': 2.1030428769017978e-05, 'epoch': 1.8464730290456433}\n",
      "Step 1790/4820 ( 37.1%) │ Loss: 0.0684 │ LR: 2.10e-05\n",
      "{'loss': 0.0684, 'grad_norm': 0.9070721864700317, 'learning_rate': 2.0961272475795295e-05, 'epoch': 1.8568464730290457}\n",
      "Step 1800/4820 ( 37.3%) │ Loss: 0.0750 │ LR: 2.09e-05\n",
      "{'loss': 0.075, 'grad_norm': 2.23844838142395, 'learning_rate': 2.0892116182572615e-05, 'epoch': 1.8672199170124482}\n",
      "Step 1810/4820 ( 37.6%) │ Loss: 0.0523 │ LR: 2.08e-05\n",
      "{'loss': 0.0523, 'grad_norm': 0.8405989408493042, 'learning_rate': 2.0822959889349932e-05, 'epoch': 1.8775933609958506}\n",
      "Step 1820/4820 ( 37.8%) │ Loss: 0.0697 │ LR: 2.08e-05\n",
      "{'loss': 0.0697, 'grad_norm': 1.0885919332504272, 'learning_rate': 2.075380359612725e-05, 'epoch': 1.887966804979253}\n",
      "Step 1830/4820 ( 38.0%) │ Loss: 0.0480 │ LR: 2.07e-05\n",
      "{'loss': 0.048, 'grad_norm': 0.9951004385948181, 'learning_rate': 2.0684647302904566e-05, 'epoch': 1.8983402489626555}\n",
      "Step 1840/4820 ( 38.2%) │ Loss: 0.0452 │ LR: 2.06e-05\n",
      "{'loss': 0.0452, 'grad_norm': 0.40225696563720703, 'learning_rate': 2.0615491009681883e-05, 'epoch': 1.908713692946058}\n",
      "Step 1850/4820 ( 38.4%) │ Loss: 0.0671 │ LR: 2.05e-05\n",
      "{'loss': 0.0671, 'grad_norm': 0.22934818267822266, 'learning_rate': 2.05463347164592e-05, 'epoch': 1.9190871369294604}\n",
      "Step 1860/4820 ( 38.6%) │ Loss: 0.0670 │ LR: 2.05e-05\n",
      "{'loss': 0.067, 'grad_norm': 1.6292088031768799, 'learning_rate': 2.0477178423236517e-05, 'epoch': 1.929460580912863}\n",
      "Step 1870/4820 ( 38.8%) │ Loss: 0.0449 │ LR: 2.04e-05\n",
      "{'loss': 0.0449, 'grad_norm': 2.296984910964966, 'learning_rate': 2.0408022130013833e-05, 'epoch': 1.9398340248962656}\n",
      "Step 1880/4820 ( 39.0%) │ Loss: 0.0477 │ LR: 2.03e-05\n",
      "{'loss': 0.0477, 'grad_norm': 1.8725740909576416, 'learning_rate': 2.033886583679115e-05, 'epoch': 1.950207468879668}\n",
      "Step 1890/4820 ( 39.2%) │ Loss: 0.0517 │ LR: 2.03e-05\n",
      "{'loss': 0.0517, 'grad_norm': 1.4748907089233398, 'learning_rate': 2.0269709543568467e-05, 'epoch': 1.9605809128630707}\n",
      "Step 1900/4820 ( 39.4%) │ Loss: 0.0467 │ LR: 2.02e-05\n",
      "{'loss': 0.0467, 'grad_norm': 1.7763841152191162, 'learning_rate': 2.0200553250345784e-05, 'epoch': 1.9709543568464731}\n",
      "Step 1910/4820 ( 39.6%) │ Loss: 0.0373 │ LR: 2.01e-05\n",
      "{'loss': 0.0373, 'grad_norm': 1.3552769422531128, 'learning_rate': 2.0131396957123098e-05, 'epoch': 1.9813278008298756}\n",
      "Step 1920/4820 ( 39.8%) │ Loss: 0.0526 │ LR: 2.01e-05\n",
      "{'loss': 0.0526, 'grad_norm': 1.1655006408691406, 'learning_rate': 2.0062240663900414e-05, 'epoch': 1.991701244813278}\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Epoch completed in 707.3s\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Validation │ F1: 0.9712 │ Precision: 0.9759 │ Recall: 0.9665\n",
      "{'eval_loss': 0.04802996665239334, 'eval_precision': 0.9758524450916151, 'eval_recall': 0.9665284538188811, 'eval_f1': 0.9711680705250127, 'eval_runtime': 56.4684, 'eval_samples_per_second': 71.899, 'eval_steps_per_second': 23.978, 'epoch': 2.0}\n",
      "\n",
      "Epoch 3/5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Step 1930/4820 ( 40.0%) │ Loss: 0.0840 │ LR: 2.00e-05\n",
      "{'loss': 0.084, 'grad_norm': 0.7980965971946716, 'learning_rate': 1.999308437067773e-05, 'epoch': 2.0020746887966805}\n",
      "Step 1940/4820 ( 40.2%) │ Loss: 0.0329 │ LR: 1.99e-05\n",
      "{'loss': 0.0329, 'grad_norm': 1.1270480155944824, 'learning_rate': 1.9923928077455048e-05, 'epoch': 2.012448132780083}\n",
      "Step 1950/4820 ( 40.5%) │ Loss: 0.0667 │ LR: 1.99e-05\n",
      "{'loss': 0.0667, 'grad_norm': 1.7505258321762085, 'learning_rate': 1.9854771784232365e-05, 'epoch': 2.0228215767634854}\n",
      "Step 1960/4820 ( 40.7%) │ Loss: 0.0300 │ LR: 1.98e-05\n",
      "{'loss': 0.03, 'grad_norm': 1.8599933385849, 'learning_rate': 1.9785615491009682e-05, 'epoch': 2.033195020746888}\n",
      "Step 1970/4820 ( 40.9%) │ Loss: 0.0576 │ LR: 1.97e-05\n",
      "{'loss': 0.0576, 'grad_norm': 1.3283027410507202, 'learning_rate': 1.9716459197787e-05, 'epoch': 2.0435684647302903}\n",
      "Step 1980/4820 ( 41.1%) │ Loss: 0.0669 │ LR: 1.96e-05\n",
      "{'loss': 0.0669, 'grad_norm': 1.8178688287734985, 'learning_rate': 1.9647302904564316e-05, 'epoch': 2.0539419087136928}\n",
      "Step 1990/4820 ( 41.3%) │ Loss: 0.0257 │ LR: 1.96e-05\n",
      "{'loss': 0.0257, 'grad_norm': 0.3512443006038666, 'learning_rate': 1.9578146611341633e-05, 'epoch': 2.064315352697095}\n",
      "Step 2000/4820 ( 41.5%) │ Loss: 0.0544 │ LR: 1.95e-05\n",
      "{'loss': 0.0544, 'grad_norm': 1.0638588666915894, 'learning_rate': 1.950899031811895e-05, 'epoch': 2.074688796680498}\n",
      "Step 2010/4820 ( 41.7%) │ Loss: 0.0588 │ LR: 1.94e-05\n",
      "{'loss': 0.0588, 'grad_norm': 2.9771835803985596, 'learning_rate': 1.9439834024896266e-05, 'epoch': 2.0850622406639006}\n",
      "Step 2020/4820 ( 41.9%) │ Loss: 0.0568 │ LR: 1.94e-05\n",
      "{'loss': 0.0568, 'grad_norm': 1.923372745513916, 'learning_rate': 1.9370677731673583e-05, 'epoch': 2.095435684647303}\n",
      "Step 2030/4820 ( 42.1%) │ Loss: 0.0435 │ LR: 1.93e-05\n",
      "{'loss': 0.0435, 'grad_norm': 1.0304973125457764, 'learning_rate': 1.93015214384509e-05, 'epoch': 2.1058091286307055}\n",
      "Step 2040/4820 ( 42.3%) │ Loss: 0.0420 │ LR: 1.92e-05\n",
      "{'loss': 0.042, 'grad_norm': 0.7233495116233826, 'learning_rate': 1.9232365145228217e-05, 'epoch': 2.116182572614108}\n",
      "Step 2050/4820 ( 42.5%) │ Loss: 0.0554 │ LR: 1.92e-05\n",
      "{'loss': 0.0554, 'grad_norm': 0.4304903447628021, 'learning_rate': 1.916320885200553e-05, 'epoch': 2.1265560165975104}\n",
      "Step 2060/4820 ( 42.7%) │ Loss: 0.0600 │ LR: 1.91e-05\n",
      "{'loss': 0.06, 'grad_norm': 0.7779541015625, 'learning_rate': 1.9094052558782847e-05, 'epoch': 2.136929460580913}\n",
      "Step 2070/4820 ( 42.9%) │ Loss: 0.0544 │ LR: 1.90e-05\n",
      "{'loss': 0.0544, 'grad_norm': 1.319211721420288, 'learning_rate': 1.9024896265560164e-05, 'epoch': 2.1473029045643153}\n",
      "Step 2080/4820 ( 43.2%) │ Loss: 0.0565 │ LR: 1.90e-05\n",
      "{'loss': 0.0565, 'grad_norm': 1.561640977859497, 'learning_rate': 1.895573997233748e-05, 'epoch': 2.1576763485477177}\n",
      "Step 2090/4820 ( 43.4%) │ Loss: 0.0505 │ LR: 1.89e-05\n",
      "{'loss': 0.0505, 'grad_norm': 1.0763332843780518, 'learning_rate': 1.8886583679114798e-05, 'epoch': 2.16804979253112}\n",
      "Step 2100/4820 ( 43.6%) │ Loss: 0.0695 │ LR: 1.88e-05\n",
      "{'loss': 0.0695, 'grad_norm': 0.9834688901901245, 'learning_rate': 1.8817427385892115e-05, 'epoch': 2.1784232365145226}\n",
      "Step 2110/4820 ( 43.8%) │ Loss: 0.0562 │ LR: 1.87e-05\n",
      "{'loss': 0.0562, 'grad_norm': 2.1185708045959473, 'learning_rate': 1.8748271092669435e-05, 'epoch': 2.1887966804979255}\n",
      "Step 2120/4820 ( 44.0%) │ Loss: 0.0609 │ LR: 1.87e-05\n",
      "{'loss': 0.0609, 'grad_norm': 0.5537803769111633, 'learning_rate': 1.8679114799446752e-05, 'epoch': 2.199170124481328}\n",
      "Step 2130/4820 ( 44.2%) │ Loss: 0.0450 │ LR: 1.86e-05\n",
      "{'loss': 0.045, 'grad_norm': 0.9359912276268005, 'learning_rate': 1.860995850622407e-05, 'epoch': 2.2095435684647304}\n",
      "Step 2140/4820 ( 44.4%) │ Loss: 0.0703 │ LR: 1.85e-05\n",
      "{'loss': 0.0703, 'grad_norm': 1.0213639736175537, 'learning_rate': 1.8540802213001386e-05, 'epoch': 2.219917012448133}\n",
      "Step 2150/4820 ( 44.6%) │ Loss: 0.0629 │ LR: 1.85e-05\n",
      "{'loss': 0.0629, 'grad_norm': 0.8711174130439758, 'learning_rate': 1.8471645919778702e-05, 'epoch': 2.2302904564315353}\n",
      "Step 2160/4820 ( 44.8%) │ Loss: 0.0517 │ LR: 1.84e-05\n",
      "{'loss': 0.0517, 'grad_norm': 1.6656981706619263, 'learning_rate': 1.840248962655602e-05, 'epoch': 2.240663900414938}\n",
      "Step 2170/4820 ( 45.0%) │ Loss: 0.0436 │ LR: 1.83e-05\n",
      "{'loss': 0.0436, 'grad_norm': 1.1846216917037964, 'learning_rate': 1.8333333333333336e-05, 'epoch': 2.2510373443983402}\n",
      "Step 2180/4820 ( 45.2%) │ Loss: 0.0356 │ LR: 1.83e-05\n",
      "{'loss': 0.0356, 'grad_norm': 0.9483994245529175, 'learning_rate': 1.826417704011065e-05, 'epoch': 2.2614107883817427}\n",
      "Step 2190/4820 ( 45.4%) │ Loss: 0.0585 │ LR: 1.82e-05\n",
      "{'loss': 0.0585, 'grad_norm': 1.0918923616409302, 'learning_rate': 1.8195020746887967e-05, 'epoch': 2.271784232365145}\n",
      "Step 2200/4820 ( 45.6%) │ Loss: 0.0435 │ LR: 1.81e-05\n",
      "{'loss': 0.0435, 'grad_norm': 1.3973695039749146, 'learning_rate': 1.8125864453665283e-05, 'epoch': 2.2821576763485476}\n",
      "Step 2210/4820 ( 45.9%) │ Loss: 0.0267 │ LR: 1.81e-05\n",
      "{'loss': 0.0267, 'grad_norm': 0.5207160711288452, 'learning_rate': 1.80567081604426e-05, 'epoch': 2.29253112033195}\n",
      "Step 2220/4820 ( 46.1%) │ Loss: 0.0389 │ LR: 1.80e-05\n",
      "{'loss': 0.0389, 'grad_norm': 0.7642324566841125, 'learning_rate': 1.7987551867219917e-05, 'epoch': 2.3029045643153525}\n",
      "Step 2230/4820 ( 46.3%) │ Loss: 0.0593 │ LR: 1.79e-05\n",
      "{'loss': 0.0593, 'grad_norm': 0.6291332840919495, 'learning_rate': 1.7918395573997234e-05, 'epoch': 2.313278008298755}\n",
      "Step 2240/4820 ( 46.5%) │ Loss: 0.0471 │ LR: 1.78e-05\n",
      "{'loss': 0.0471, 'grad_norm': 0.7949427962303162, 'learning_rate': 1.784923928077455e-05, 'epoch': 2.323651452282158}\n",
      "Step 2250/4820 ( 46.7%) │ Loss: 0.0483 │ LR: 1.78e-05\n",
      "{'loss': 0.0483, 'grad_norm': 0.8887600898742676, 'learning_rate': 1.7780082987551868e-05, 'epoch': 2.3340248962655603}\n",
      "Step 2260/4820 ( 46.9%) │ Loss: 0.0382 │ LR: 1.77e-05\n",
      "{'loss': 0.0382, 'grad_norm': 1.8014564514160156, 'learning_rate': 1.7710926694329185e-05, 'epoch': 2.3443983402489628}\n",
      "Step 2270/4820 ( 47.1%) │ Loss: 0.0574 │ LR: 1.76e-05\n",
      "{'loss': 0.0574, 'grad_norm': 1.524063229560852, 'learning_rate': 1.76417704011065e-05, 'epoch': 2.354771784232365}\n",
      "Step 2280/4820 ( 47.3%) │ Loss: 0.0481 │ LR: 1.76e-05\n",
      "{'loss': 0.0481, 'grad_norm': 1.4316155910491943, 'learning_rate': 1.757261410788382e-05, 'epoch': 2.3651452282157677}\n",
      "Step 2290/4820 ( 47.5%) │ Loss: 0.0499 │ LR: 1.75e-05\n",
      "{'loss': 0.0499, 'grad_norm': 1.404672622680664, 'learning_rate': 1.7503457814661135e-05, 'epoch': 2.37551867219917}\n",
      "Step 2300/4820 ( 47.7%) │ Loss: 0.0440 │ LR: 1.74e-05\n",
      "{'loss': 0.044, 'grad_norm': 1.0398359298706055, 'learning_rate': 1.7434301521438452e-05, 'epoch': 2.3858921161825726}\n",
      "Step 2310/4820 ( 47.9%) │ Loss: 0.0453 │ LR: 1.74e-05\n",
      "{'loss': 0.0453, 'grad_norm': 0.615900993347168, 'learning_rate': 1.736514522821577e-05, 'epoch': 2.396265560165975}\n",
      "Step 2320/4820 ( 48.1%) │ Loss: 0.0474 │ LR: 1.73e-05\n",
      "{'loss': 0.0474, 'grad_norm': 1.379590630531311, 'learning_rate': 1.7295988934993083e-05, 'epoch': 2.4066390041493775}\n",
      "Step 2330/4820 ( 48.3%) │ Loss: 0.0341 │ LR: 1.72e-05\n",
      "{'loss': 0.0341, 'grad_norm': 0.14452563226222992, 'learning_rate': 1.72268326417704e-05, 'epoch': 2.41701244813278}\n",
      "Step 2340/4820 ( 48.5%) │ Loss: 0.0485 │ LR: 1.72e-05\n",
      "{'loss': 0.0485, 'grad_norm': 0.8934617042541504, 'learning_rate': 1.7157676348547716e-05, 'epoch': 2.4273858921161824}\n",
      "Step 2350/4820 ( 48.8%) │ Loss: 0.0507 │ LR: 1.71e-05\n",
      "{'loss': 0.0507, 'grad_norm': 1.4785676002502441, 'learning_rate': 1.7088520055325033e-05, 'epoch': 2.4377593360995853}\n",
      "Step 2360/4820 ( 49.0%) │ Loss: 0.0266 │ LR: 1.70e-05\n",
      "{'loss': 0.0266, 'grad_norm': 0.707076907157898, 'learning_rate': 1.701936376210235e-05, 'epoch': 2.4481327800829877}\n",
      "Step 2370/4820 ( 49.2%) │ Loss: 0.0319 │ LR: 1.70e-05\n",
      "{'loss': 0.0319, 'grad_norm': 1.6123219728469849, 'learning_rate': 1.6950207468879667e-05, 'epoch': 2.45850622406639}\n",
      "Step 2380/4820 ( 49.4%) │ Loss: 0.0452 │ LR: 1.69e-05\n",
      "{'loss': 0.0452, 'grad_norm': 1.3734173774719238, 'learning_rate': 1.6881051175656984e-05, 'epoch': 2.4688796680497926}\n",
      "Step 2390/4820 ( 49.6%) │ Loss: 0.0298 │ LR: 1.68e-05\n",
      "{'loss': 0.0298, 'grad_norm': 0.6098060011863708, 'learning_rate': 1.68118948824343e-05, 'epoch': 2.479253112033195}\n",
      "Step 2400/4820 ( 49.8%) │ Loss: 0.0319 │ LR: 1.67e-05\n",
      "{'loss': 0.0319, 'grad_norm': 0.2731265723705292, 'learning_rate': 1.6742738589211618e-05, 'epoch': 2.4896265560165975}\n",
      "Step 2410/4820 ( 50.0%) │ Loss: 0.0427 │ LR: 1.67e-05\n",
      "{'loss': 0.0427, 'grad_norm': 1.00260329246521, 'learning_rate': 1.6673582295988938e-05, 'epoch': 2.5}\n",
      "Step 2420/4820 ( 50.2%) │ Loss: 0.0292 │ LR: 1.66e-05\n",
      "{'loss': 0.0292, 'grad_norm': 0.32932180166244507, 'learning_rate': 1.6604426002766255e-05, 'epoch': 2.5103734439834025}\n",
      "Step 2430/4820 ( 50.4%) │ Loss: 0.0343 │ LR: 1.65e-05\n",
      "{'loss': 0.0343, 'grad_norm': 2.330293893814087, 'learning_rate': 1.653526970954357e-05, 'epoch': 2.520746887966805}\n",
      "Step 2440/4820 ( 50.6%) │ Loss: 0.0424 │ LR: 1.65e-05\n",
      "{'loss': 0.0424, 'grad_norm': 1.1294687986373901, 'learning_rate': 1.646611341632089e-05, 'epoch': 2.5311203319502074}\n",
      "Step 2450/4820 ( 50.8%) │ Loss: 0.0620 │ LR: 1.64e-05\n",
      "{'loss': 0.062, 'grad_norm': 0.8731338977813721, 'learning_rate': 1.6396957123098202e-05, 'epoch': 2.54149377593361}\n",
      "Step 2460/4820 ( 51.0%) │ Loss: 0.1055 │ LR: 1.63e-05\n",
      "{'loss': 0.1055, 'grad_norm': 0.22594547271728516, 'learning_rate': 1.632780082987552e-05, 'epoch': 2.5518672199170123}\n",
      "Step 2470/4820 ( 51.2%) │ Loss: 0.0374 │ LR: 1.63e-05\n",
      "{'loss': 0.0374, 'grad_norm': 0.6002386212348938, 'learning_rate': 1.6258644536652836e-05, 'epoch': 2.5622406639004147}\n",
      "Step 2480/4820 ( 51.5%) │ Loss: 0.0459 │ LR: 1.62e-05\n",
      "{'loss': 0.0459, 'grad_norm': 1.4344004392623901, 'learning_rate': 1.6189488243430153e-05, 'epoch': 2.572614107883817}\n",
      "Step 2490/4820 ( 51.7%) │ Loss: 0.0342 │ LR: 1.61e-05\n",
      "{'loss': 0.0342, 'grad_norm': 4.248064994812012, 'learning_rate': 1.612033195020747e-05, 'epoch': 2.58298755186722}\n",
      "Step 2500/4820 ( 51.9%) │ Loss: 0.0434 │ LR: 1.61e-05\n",
      "{'loss': 0.0434, 'grad_norm': 0.6658688187599182, 'learning_rate': 1.6051175656984786e-05, 'epoch': 2.5933609958506225}\n",
      "Step 2510/4820 ( 52.1%) │ Loss: 0.0467 │ LR: 1.60e-05\n",
      "{'loss': 0.0467, 'grad_norm': 0.901055097579956, 'learning_rate': 1.5982019363762103e-05, 'epoch': 2.603734439834025}\n",
      "Step 2520/4820 ( 52.3%) │ Loss: 0.0661 │ LR: 1.59e-05\n",
      "{'loss': 0.0661, 'grad_norm': 3.5517027378082275, 'learning_rate': 1.591286307053942e-05, 'epoch': 2.6141078838174274}\n",
      "Step 2530/4820 ( 52.5%) │ Loss: 0.0599 │ LR: 1.58e-05\n",
      "{'loss': 0.0599, 'grad_norm': 1.9916623830795288, 'learning_rate': 1.5843706777316737e-05, 'epoch': 2.62448132780083}\n",
      "Step 2540/4820 ( 52.7%) │ Loss: 0.0295 │ LR: 1.58e-05\n",
      "{'loss': 0.0295, 'grad_norm': 1.193762183189392, 'learning_rate': 1.5774550484094054e-05, 'epoch': 2.6348547717842323}\n",
      "Step 2550/4820 ( 52.9%) │ Loss: 0.0209 │ LR: 1.57e-05\n",
      "{'loss': 0.0209, 'grad_norm': 1.1436587572097778, 'learning_rate': 1.570539419087137e-05, 'epoch': 2.645228215767635}\n",
      "Step 2560/4820 ( 53.1%) │ Loss: 0.0377 │ LR: 1.56e-05\n",
      "{'loss': 0.0377, 'grad_norm': 1.953706979751587, 'learning_rate': 1.5636237897648688e-05, 'epoch': 2.6556016597510372}\n",
      "Step 2570/4820 ( 53.3%) │ Loss: 0.0424 │ LR: 1.56e-05\n",
      "{'loss': 0.0424, 'grad_norm': 2.4870588779449463, 'learning_rate': 1.5567081604426004e-05, 'epoch': 2.6659751037344397}\n",
      "Step 2580/4820 ( 53.5%) │ Loss: 0.0285 │ LR: 1.55e-05\n",
      "{'loss': 0.0285, 'grad_norm': inf, 'learning_rate': 1.549792531120332e-05, 'epoch': 2.6763485477178426}\n",
      "Step 2590/4820 ( 53.7%) │ Loss: 0.0411 │ LR: 1.54e-05\n",
      "{'loss': 0.0411, 'grad_norm': 1.3251243829727173, 'learning_rate': 1.5435684647302905e-05, 'epoch': 2.686721991701245}\n",
      "Step 2600/4820 ( 53.9%) │ Loss: 0.0132 │ LR: 1.54e-05\n",
      "{'loss': 0.0132, 'grad_norm': 0.4325391352176666, 'learning_rate': 1.5366528354080222e-05, 'epoch': 2.6970954356846475}\n",
      "Step 2610/4820 ( 54.1%) │ Loss: 0.0363 │ LR: 1.53e-05\n",
      "{'loss': 0.0363, 'grad_norm': 0.44077739119529724, 'learning_rate': 1.529737206085754e-05, 'epoch': 2.70746887966805}\n",
      "Step 2620/4820 ( 54.4%) │ Loss: 0.0530 │ LR: 1.52e-05\n",
      "{'loss': 0.053, 'grad_norm': 1.413573980331421, 'learning_rate': 1.5228215767634854e-05, 'epoch': 2.7178423236514524}\n",
      "Step 2630/4820 ( 54.6%) │ Loss: 0.0400 │ LR: 1.52e-05\n",
      "{'loss': 0.04, 'grad_norm': 0.17383162677288055, 'learning_rate': 1.5159059474412171e-05, 'epoch': 2.728215767634855}\n",
      "Step 2640/4820 ( 54.8%) │ Loss: 0.0403 │ LR: 1.51e-05\n",
      "{'loss': 0.0403, 'grad_norm': 1.2327102422714233, 'learning_rate': 1.5089903181189488e-05, 'epoch': 2.7385892116182573}\n",
      "Step 2650/4820 ( 55.0%) │ Loss: 0.0288 │ LR: 1.50e-05\n",
      "{'loss': 0.0288, 'grad_norm': 1.2558581829071045, 'learning_rate': 1.5020746887966805e-05, 'epoch': 2.7489626556016598}\n",
      "Step 2660/4820 ( 55.2%) │ Loss: 0.0481 │ LR: 1.50e-05\n",
      "{'loss': 0.0481, 'grad_norm': 1.358036994934082, 'learning_rate': 1.4951590594744122e-05, 'epoch': 2.759336099585062}\n",
      "Step 2670/4820 ( 55.4%) │ Loss: 0.0406 │ LR: 1.49e-05\n",
      "{'loss': 0.0406, 'grad_norm': 3.262373924255371, 'learning_rate': 1.488243430152144e-05, 'epoch': 2.7697095435684647}\n",
      "Step 2680/4820 ( 55.6%) │ Loss: 0.0352 │ LR: 1.48e-05\n",
      "{'loss': 0.0352, 'grad_norm': 1.1188889741897583, 'learning_rate': 1.4813278008298757e-05, 'epoch': 2.780082987551867}\n",
      "Step 2690/4820 ( 55.8%) │ Loss: 0.0392 │ LR: 1.47e-05\n",
      "{'loss': 0.0392, 'grad_norm': 1.2584446668624878, 'learning_rate': 1.4744121715076072e-05, 'epoch': 2.7904564315352696}\n",
      "Step 2700/4820 ( 56.0%) │ Loss: 0.0413 │ LR: 1.47e-05\n",
      "{'loss': 0.0413, 'grad_norm': 1.0625065565109253, 'learning_rate': 1.4674965421853389e-05, 'epoch': 2.800829875518672}\n",
      "Step 2710/4820 ( 56.2%) │ Loss: 0.0366 │ LR: 1.46e-05\n",
      "{'loss': 0.0366, 'grad_norm': 1.3721113204956055, 'learning_rate': 1.4605809128630706e-05, 'epoch': 2.8112033195020745}\n",
      "Step 2720/4820 ( 56.4%) │ Loss: 0.0347 │ LR: 1.45e-05\n",
      "{'loss': 0.0347, 'grad_norm': 1.6349955797195435, 'learning_rate': 1.4536652835408023e-05, 'epoch': 2.821576763485477}\n",
      "Step 2730/4820 ( 56.6%) │ Loss: 0.0324 │ LR: 1.45e-05\n",
      "{'loss': 0.0324, 'grad_norm': 1.9660335779190063, 'learning_rate': 1.446749654218534e-05, 'epoch': 2.83195020746888}\n",
      "Step 2740/4820 ( 56.8%) │ Loss: 0.0409 │ LR: 1.44e-05\n",
      "{'loss': 0.0409, 'grad_norm': 1.8559948205947876, 'learning_rate': 1.4398340248962656e-05, 'epoch': 2.8423236514522823}\n",
      "Step 2750/4820 ( 57.1%) │ Loss: 0.0578 │ LR: 1.43e-05\n",
      "{'loss': 0.0578, 'grad_norm': 1.4695638418197632, 'learning_rate': 1.4329183955739973e-05, 'epoch': 2.8526970954356847}\n",
      "Step 2760/4820 ( 57.3%) │ Loss: 0.0466 │ LR: 1.43e-05\n",
      "{'loss': 0.0466, 'grad_norm': 0.5386757254600525, 'learning_rate': 1.4260027662517289e-05, 'epoch': 2.863070539419087}\n",
      "Step 2770/4820 ( 57.5%) │ Loss: 0.0154 │ LR: 1.42e-05\n",
      "{'loss': 0.0154, 'grad_norm': 0.3526797592639923, 'learning_rate': 1.4190871369294605e-05, 'epoch': 2.8734439834024896}\n",
      "Step 2780/4820 ( 57.7%) │ Loss: 0.0337 │ LR: 1.41e-05\n",
      "{'loss': 0.0337, 'grad_norm': 1.6434738636016846, 'learning_rate': 1.4121715076071922e-05, 'epoch': 2.883817427385892}\n",
      "Step 2790/4820 ( 57.9%) │ Loss: 0.0176 │ LR: 1.41e-05\n",
      "{'loss': 0.0176, 'grad_norm': 1.0029048919677734, 'learning_rate': 1.405255878284924e-05, 'epoch': 2.8941908713692945}\n",
      "Step 2800/4820 ( 58.1%) │ Loss: 0.0239 │ LR: 1.40e-05\n",
      "{'loss': 0.0239, 'grad_norm': 0.5647125840187073, 'learning_rate': 1.3983402489626556e-05, 'epoch': 2.904564315352697}\n",
      "Step 2810/4820 ( 58.3%) │ Loss: 0.0440 │ LR: 1.39e-05\n",
      "{'loss': 0.044, 'grad_norm': 2.0234124660491943, 'learning_rate': 1.3914246196403873e-05, 'epoch': 2.9149377593360994}\n",
      "Step 2820/4820 ( 58.5%) │ Loss: 0.0635 │ LR: 1.38e-05\n",
      "{'loss': 0.0635, 'grad_norm': 1.2685619592666626, 'learning_rate': 1.384508990318119e-05, 'epoch': 2.9253112033195023}\n",
      "Step 2830/4820 ( 58.7%) │ Loss: 0.0547 │ LR: 1.38e-05\n",
      "{'loss': 0.0547, 'grad_norm': 1.4929946660995483, 'learning_rate': 1.3775933609958507e-05, 'epoch': 2.935684647302905}\n",
      "Step 2840/4820 ( 58.9%) │ Loss: 0.0475 │ LR: 1.37e-05\n",
      "{'loss': 0.0475, 'grad_norm': 1.8383946418762207, 'learning_rate': 1.3706777316735824e-05, 'epoch': 2.9460580912863072}\n",
      "Step 2850/4820 ( 59.1%) │ Loss: 0.0326 │ LR: 1.36e-05\n",
      "{'loss': 0.0326, 'grad_norm': 0.5940648317337036, 'learning_rate': 1.363762102351314e-05, 'epoch': 2.9564315352697097}\n",
      "Step 2860/4820 ( 59.3%) │ Loss: 0.0423 │ LR: 1.36e-05\n",
      "{'loss': 0.0423, 'grad_norm': 0.6982638835906982, 'learning_rate': 1.3568464730290457e-05, 'epoch': 2.966804979253112}\n",
      "Step 2870/4820 ( 59.5%) │ Loss: 0.0311 │ LR: 1.35e-05\n",
      "{'loss': 0.0311, 'grad_norm': 0.47983503341674805, 'learning_rate': 1.3499308437067774e-05, 'epoch': 2.9771784232365146}\n",
      "Step 2880/4820 ( 59.8%) │ Loss: 0.0272 │ LR: 1.34e-05\n",
      "{'loss': 0.0272, 'grad_norm': 1.740950107574463, 'learning_rate': 1.3430152143845091e-05, 'epoch': 2.987551867219917}\n",
      "Step 2890/4820 ( 60.0%) │ Loss: 0.0499 │ LR: 1.34e-05\n",
      "{'loss': 0.0499, 'grad_norm': 1.039899230003357, 'learning_rate': 1.3360995850622408e-05, 'epoch': 2.9979253112033195}\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Epoch completed in 705.9s\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Validation │ F1: 0.9774 │ Precision: 0.9800 │ Recall: 0.9748\n",
      "{'eval_loss': 0.03927186504006386, 'eval_precision': 0.9800036247205944, 'eval_recall': 0.9748212246860165, 'eval_f1': 0.9774055552208231, 'eval_runtime': 56.6204, 'eval_samples_per_second': 71.706, 'eval_steps_per_second': 23.914, 'epoch': 3.0}\n",
      "\n",
      "Epoch 4/5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Step 2900/4820 ( 60.2%) │ Loss: 0.0554 │ LR: 1.33e-05\n",
      "{'loss': 0.0554, 'grad_norm': 0.3756396770477295, 'learning_rate': 1.3291839557399723e-05, 'epoch': 3.008298755186722}\n",
      "Step 2910/4820 ( 60.4%) │ Loss: 0.0198 │ LR: 1.32e-05\n",
      "{'loss': 0.0198, 'grad_norm': 0.8426598906517029, 'learning_rate': 1.322268326417704e-05, 'epoch': 3.0186721991701244}\n",
      "Step 2920/4820 ( 60.6%) │ Loss: 0.0281 │ LR: 1.32e-05\n",
      "{'loss': 0.0281, 'grad_norm': 0.945881724357605, 'learning_rate': 1.3153526970954357e-05, 'epoch': 3.029045643153527}\n",
      "Step 2930/4820 ( 60.8%) │ Loss: 0.0292 │ LR: 1.31e-05\n",
      "{'loss': 0.0292, 'grad_norm': 0.6041212677955627, 'learning_rate': 1.3084370677731674e-05, 'epoch': 3.0394190871369293}\n",
      "Step 2940/4820 ( 61.0%) │ Loss: 0.0326 │ LR: 1.30e-05\n",
      "{'loss': 0.0326, 'grad_norm': 1.357096552848816, 'learning_rate': 1.301521438450899e-05, 'epoch': 3.0497925311203318}\n",
      "Step 2950/4820 ( 61.2%) │ Loss: 0.0229 │ LR: 1.29e-05\n",
      "{'loss': 0.0229, 'grad_norm': 0.7897115349769592, 'learning_rate': 1.2946058091286307e-05, 'epoch': 3.0601659751037342}\n",
      "Step 2960/4820 ( 61.4%) │ Loss: 0.0123 │ LR: 1.29e-05\n",
      "{'loss': 0.0123, 'grad_norm': 0.1894213706254959, 'learning_rate': 1.2876901798063623e-05, 'epoch': 3.070539419087137}\n",
      "Step 2970/4820 ( 61.6%) │ Loss: 0.0294 │ LR: 1.28e-05\n",
      "{'loss': 0.0294, 'grad_norm': 0.7258840799331665, 'learning_rate': 1.280774550484094e-05, 'epoch': 3.0809128630705396}\n",
      "Step 2980/4820 ( 61.8%) │ Loss: 0.0372 │ LR: 1.27e-05\n",
      "{'loss': 0.0372, 'grad_norm': 1.1756150722503662, 'learning_rate': 1.2738589211618258e-05, 'epoch': 3.091286307053942}\n",
      "Step 2990/4820 ( 62.0%) │ Loss: 0.0199 │ LR: 1.27e-05\n",
      "{'loss': 0.0199, 'grad_norm': 0.7551348209381104, 'learning_rate': 1.2669432918395575e-05, 'epoch': 3.1016597510373445}\n",
      "Step 3000/4820 ( 62.2%) │ Loss: 0.0207 │ LR: 1.26e-05\n",
      "{'loss': 0.0207, 'grad_norm': 0.2854519784450531, 'learning_rate': 1.2600276625172892e-05, 'epoch': 3.112033195020747}\n",
      "Step 3010/4820 ( 62.4%) │ Loss: 0.0340 │ LR: 1.25e-05\n",
      "{'loss': 0.034, 'grad_norm': 1.7306615114212036, 'learning_rate': 1.2531120331950209e-05, 'epoch': 3.1224066390041494}\n",
      "Step 3020/4820 ( 62.7%) │ Loss: 0.0362 │ LR: 1.25e-05\n",
      "{'loss': 0.0362, 'grad_norm': 0.19408510625362396, 'learning_rate': 1.2461964038727526e-05, 'epoch': 3.132780082987552}\n",
      "Step 3030/4820 ( 62.9%) │ Loss: 0.0221 │ LR: 1.24e-05\n",
      "{'loss': 0.0221, 'grad_norm': 1.2334377765655518, 'learning_rate': 1.239280774550484e-05, 'epoch': 3.1431535269709543}\n",
      "Step 3040/4820 ( 63.1%) │ Loss: 0.0392 │ LR: 1.23e-05\n",
      "{'loss': 0.0392, 'grad_norm': 0.8637084364891052, 'learning_rate': 1.2323651452282158e-05, 'epoch': 3.1535269709543567}\n",
      "Step 3050/4820 ( 63.3%) │ Loss: 0.0309 │ LR: 1.23e-05\n",
      "{'loss': 0.0309, 'grad_norm': 1.7494258880615234, 'learning_rate': 1.2254495159059474e-05, 'epoch': 3.163900414937759}\n",
      "Step 3060/4820 ( 63.5%) │ Loss: 0.0241 │ LR: 1.22e-05\n",
      "{'loss': 0.0241, 'grad_norm': 0.17134544253349304, 'learning_rate': 1.2185338865836791e-05, 'epoch': 3.1742738589211617}\n",
      "Step 3070/4820 ( 63.7%) │ Loss: 0.0248 │ LR: 1.21e-05\n",
      "{'loss': 0.0248, 'grad_norm': 0.24635234475135803, 'learning_rate': 1.2116182572614108e-05, 'epoch': 3.1846473029045645}\n",
      "Step 3080/4820 ( 63.9%) │ Loss: 0.0276 │ LR: 1.20e-05\n",
      "{'loss': 0.0276, 'grad_norm': 1.798568606376648, 'learning_rate': 1.2047026279391425e-05, 'epoch': 3.195020746887967}\n",
      "Step 3090/4820 ( 64.1%) │ Loss: 0.0303 │ LR: 1.20e-05\n",
      "{'loss': 0.0303, 'grad_norm': 1.201445460319519, 'learning_rate': 1.1977869986168742e-05, 'epoch': 3.2053941908713695}\n",
      "Step 3100/4820 ( 64.3%) │ Loss: 0.0163 │ LR: 1.19e-05\n",
      "{'loss': 0.0163, 'grad_norm': 1.4187020063400269, 'learning_rate': 1.1908713692946057e-05, 'epoch': 3.215767634854772}\n",
      "Step 3110/4820 ( 64.5%) │ Loss: 0.0260 │ LR: 1.18e-05\n",
      "{'loss': 0.026, 'grad_norm': 0.09135852009057999, 'learning_rate': 1.1839557399723374e-05, 'epoch': 3.2261410788381744}\n",
      "Step 3120/4820 ( 64.7%) │ Loss: 0.0245 │ LR: 1.18e-05\n",
      "{'loss': 0.0245, 'grad_norm': 2.689439058303833, 'learning_rate': 1.1770401106500691e-05, 'epoch': 3.236514522821577}\n",
      "Step 3130/4820 ( 64.9%) │ Loss: 0.0450 │ LR: 1.17e-05\n",
      "{'loss': 0.045, 'grad_norm': 1.4632492065429688, 'learning_rate': 1.170124481327801e-05, 'epoch': 3.2468879668049793}\n",
      "Step 3140/4820 ( 65.1%) │ Loss: 0.0348 │ LR: 1.16e-05\n",
      "{'loss': 0.0348, 'grad_norm': 1.8565733432769775, 'learning_rate': 1.1632088520055326e-05, 'epoch': 3.2572614107883817}\n",
      "Step 3150/4820 ( 65.4%) │ Loss: 0.0281 │ LR: 1.16e-05\n",
      "{'loss': 0.0281, 'grad_norm': 1.656636357307434, 'learning_rate': 1.1562932226832643e-05, 'epoch': 3.267634854771784}\n",
      "Step 3160/4820 ( 65.6%) │ Loss: 0.0436 │ LR: 1.15e-05\n",
      "{'loss': 0.0436, 'grad_norm': 1.3135097026824951, 'learning_rate': 1.149377593360996e-05, 'epoch': 3.2780082987551866}\n",
      "Step 3170/4820 ( 65.8%) │ Loss: 0.0244 │ LR: 1.14e-05\n",
      "{'loss': 0.0244, 'grad_norm': 0.6428038477897644, 'learning_rate': 1.1424619640387275e-05, 'epoch': 3.288381742738589}\n",
      "Step 3180/4820 ( 66.0%) │ Loss: 0.0268 │ LR: 1.14e-05\n",
      "{'loss': 0.0268, 'grad_norm': 0.17757952213287354, 'learning_rate': 1.1355463347164592e-05, 'epoch': 3.2987551867219915}\n",
      "Step 3190/4820 ( 66.2%) │ Loss: 0.0201 │ LR: 1.13e-05\n",
      "{'loss': 0.0201, 'grad_norm': 2.125525712966919, 'learning_rate': 1.1286307053941909e-05, 'epoch': 3.309128630705394}\n",
      "Step 3200/4820 ( 66.4%) │ Loss: 0.0279 │ LR: 1.12e-05\n",
      "{'loss': 0.0279, 'grad_norm': 0.947048008441925, 'learning_rate': 1.1217150760719226e-05, 'epoch': 3.3195020746887964}\n",
      "Step 3210/4820 ( 66.6%) │ Loss: 0.0448 │ LR: 1.11e-05\n",
      "{'loss': 0.0448, 'grad_norm': 4.3836669921875, 'learning_rate': 1.1147994467496543e-05, 'epoch': 3.3298755186721993}\n",
      "Step 3220/4820 ( 66.8%) │ Loss: 0.0205 │ LR: 1.11e-05\n",
      "{'loss': 0.0205, 'grad_norm': 0.31276407837867737, 'learning_rate': 1.107883817427386e-05, 'epoch': 3.340248962655602}\n",
      "Step 3230/4820 ( 67.0%) │ Loss: 0.0114 │ LR: 1.10e-05\n",
      "{'loss': 0.0114, 'grad_norm': 0.23377306759357452, 'learning_rate': 1.1009681881051175e-05, 'epoch': 3.3506224066390042}\n",
      "Step 3240/4820 ( 67.2%) │ Loss: 0.0443 │ LR: 1.09e-05\n",
      "{'loss': 0.0443, 'grad_norm': 1.2304675579071045, 'learning_rate': 1.0940525587828492e-05, 'epoch': 3.3609958506224067}\n",
      "Step 3250/4820 ( 67.4%) │ Loss: 0.0243 │ LR: 1.09e-05\n",
      "{'loss': 0.0243, 'grad_norm': 0.7707324028015137, 'learning_rate': 1.0871369294605809e-05, 'epoch': 3.371369294605809}\n",
      "Step 3260/4820 ( 67.6%) │ Loss: 0.0230 │ LR: 1.08e-05\n",
      "{'loss': 0.023, 'grad_norm': 0.5758213400840759, 'learning_rate': 1.0802213001383125e-05, 'epoch': 3.3817427385892116}\n",
      "Step 3270/4820 ( 67.8%) │ Loss: 0.0239 │ LR: 1.07e-05\n",
      "{'loss': 0.0239, 'grad_norm': 2.077075719833374, 'learning_rate': 1.0733056708160442e-05, 'epoch': 3.392116182572614}\n",
      "Step 3280/4820 ( 68.0%) │ Loss: 0.0211 │ LR: 1.07e-05\n",
      "{'loss': 0.0211, 'grad_norm': 0.5323304533958435, 'learning_rate': 1.0663900414937761e-05, 'epoch': 3.4024896265560165}\n",
      "Step 3290/4820 ( 68.3%) │ Loss: 0.0498 │ LR: 1.06e-05\n",
      "{'loss': 0.0498, 'grad_norm': 2.2806036472320557, 'learning_rate': 1.0594744121715078e-05, 'epoch': 3.412863070539419}\n",
      "Step 3300/4820 ( 68.5%) │ Loss: 0.0298 │ LR: 1.05e-05\n",
      "{'loss': 0.0298, 'grad_norm': 1.2305448055267334, 'learning_rate': 1.0525587828492393e-05, 'epoch': 3.4232365145228214}\n",
      "Step 3310/4820 ( 68.7%) │ Loss: 0.0422 │ LR: 1.05e-05\n",
      "{'loss': 0.0422, 'grad_norm': 1.99082350730896, 'learning_rate': 1.045643153526971e-05, 'epoch': 3.4336099585062243}\n",
      "Step 3320/4820 ( 68.9%) │ Loss: 0.0365 │ LR: 1.04e-05\n",
      "{'loss': 0.0365, 'grad_norm': 1.7457232475280762, 'learning_rate': 1.0387275242047027e-05, 'epoch': 3.4439834024896268}\n",
      "Step 3330/4820 ( 69.1%) │ Loss: 0.0335 │ LR: 1.03e-05\n",
      "{'loss': 0.0335, 'grad_norm': 1.1542969942092896, 'learning_rate': 1.0318118948824344e-05, 'epoch': 3.454356846473029}\n",
      "Step 3340/4820 ( 69.3%) │ Loss: 0.0195 │ LR: 1.02e-05\n",
      "{'loss': 0.0195, 'grad_norm': 1.0144222974777222, 'learning_rate': 1.024896265560166e-05, 'epoch': 3.4647302904564317}\n",
      "Step 3350/4820 ( 69.5%) │ Loss: 0.0225 │ LR: 1.02e-05\n",
      "{'loss': 0.0225, 'grad_norm': 0.8716764450073242, 'learning_rate': 1.0179806362378977e-05, 'epoch': 3.475103734439834}\n",
      "Step 3360/4820 ( 69.7%) │ Loss: 0.0242 │ LR: 1.01e-05\n",
      "{'loss': 0.0242, 'grad_norm': 2.591454029083252, 'learning_rate': 1.0110650069156294e-05, 'epoch': 3.4854771784232366}\n",
      "Step 3370/4820 ( 69.9%) │ Loss: 0.0171 │ LR: 1.00e-05\n",
      "{'loss': 0.0171, 'grad_norm': 1.4372446537017822, 'learning_rate': 1.004149377593361e-05, 'epoch': 3.495850622406639}\n",
      "Step 3380/4820 ( 70.1%) │ Loss: 0.0237 │ LR: 9.97e-06\n",
      "{'loss': 0.0237, 'grad_norm': 1.7109649181365967, 'learning_rate': 9.972337482710926e-06, 'epoch': 3.5062240663900415}\n",
      "Step 3390/4820 ( 70.3%) │ Loss: 0.0293 │ LR: 9.90e-06\n",
      "{'loss': 0.0293, 'grad_norm': 0.011644117534160614, 'learning_rate': 9.903181189488243e-06, 'epoch': 3.516597510373444}\n",
      "Step 3400/4820 ( 70.5%) │ Loss: 0.0285 │ LR: 9.83e-06\n",
      "{'loss': 0.0285, 'grad_norm': 1.2023195028305054, 'learning_rate': 9.83402489626556e-06, 'epoch': 3.5269709543568464}\n",
      "Step 3410/4820 ( 70.7%) │ Loss: 0.0269 │ LR: 9.76e-06\n",
      "{'loss': 0.0269, 'grad_norm': 0.5604777336120605, 'learning_rate': 9.764868603042877e-06, 'epoch': 3.537344398340249}\n",
      "Step 3420/4820 ( 71.0%) │ Loss: 0.0201 │ LR: 9.70e-06\n",
      "{'loss': 0.0201, 'grad_norm': 0.08743367344141006, 'learning_rate': 9.695712309820194e-06, 'epoch': 3.5477178423236513}\n",
      "Step 3430/4820 ( 71.2%) │ Loss: 0.0289 │ LR: 9.63e-06\n",
      "{'loss': 0.0289, 'grad_norm': 0.26343604922294617, 'learning_rate': 9.62655601659751e-06, 'epoch': 3.5580912863070537}\n",
      "Step 3440/4820 ( 71.4%) │ Loss: 0.0214 │ LR: 9.56e-06\n",
      "{'loss': 0.0214, 'grad_norm': 2.642153024673462, 'learning_rate': 9.557399723374827e-06, 'epoch': 3.568464730290456}\n",
      "Step 3450/4820 ( 71.6%) │ Loss: 0.0478 │ LR: 9.49e-06\n",
      "{'loss': 0.0478, 'grad_norm': 1.0589889287948608, 'learning_rate': 9.488243430152144e-06, 'epoch': 3.578838174273859}\n",
      "Step 3460/4820 ( 71.8%) │ Loss: 0.0361 │ LR: 9.42e-06\n",
      "{'loss': 0.0361, 'grad_norm': 0.8444734215736389, 'learning_rate': 9.419087136929461e-06, 'epoch': 3.5892116182572615}\n",
      "Step 3470/4820 ( 72.0%) │ Loss: 0.0167 │ LR: 9.35e-06\n",
      "{'loss': 0.0167, 'grad_norm': 2.568368673324585, 'learning_rate': 9.349930843706778e-06, 'epoch': 3.599585062240664}\n",
      "Step 3480/4820 ( 72.2%) │ Loss: 0.0427 │ LR: 9.28e-06\n",
      "{'loss': 0.0427, 'grad_norm': 1.0203197002410889, 'learning_rate': 9.280774550484095e-06, 'epoch': 3.6099585062240664}\n",
      "Step 3490/4820 ( 72.4%) │ Loss: 0.0270 │ LR: 9.21e-06\n",
      "{'loss': 0.027, 'grad_norm': 0.39008766412734985, 'learning_rate': 9.211618257261412e-06, 'epoch': 3.620331950207469}\n",
      "Step 3500/4820 ( 72.6%) │ Loss: 0.0478 │ LR: 9.14e-06\n",
      "{'loss': 0.0478, 'grad_norm': 2.302783966064453, 'learning_rate': 9.142461964038729e-06, 'epoch': 3.6307053941908713}\n",
      "Step 3510/4820 ( 72.8%) │ Loss: 0.0238 │ LR: 9.07e-06\n",
      "{'loss': 0.0238, 'grad_norm': 0.7134355306625366, 'learning_rate': 9.073305670816044e-06, 'epoch': 3.641078838174274}\n",
      "Step 3520/4820 ( 73.0%) │ Loss: 0.0117 │ LR: 9.00e-06\n",
      "{'loss': 0.0117, 'grad_norm': 0.7406846880912781, 'learning_rate': 9.00414937759336e-06, 'epoch': 3.6514522821576763}\n",
      "Step 3530/4820 ( 73.2%) │ Loss: 0.0157 │ LR: 8.93e-06\n",
      "{'loss': 0.0157, 'grad_norm': 0.1261865645647049, 'learning_rate': 8.934993084370678e-06, 'epoch': 3.6618257261410787}\n",
      "Step 3540/4820 ( 73.4%) │ Loss: 0.0226 │ LR: 8.87e-06\n",
      "{'loss': 0.0226, 'grad_norm': 0.9197095632553101, 'learning_rate': 8.865836791147994e-06, 'epoch': 3.6721991701244816}\n",
      "Step 3550/4820 ( 73.7%) │ Loss: 0.0116 │ LR: 8.80e-06\n",
      "{'loss': 0.0116, 'grad_norm': 0.059216778725385666, 'learning_rate': 8.796680497925311e-06, 'epoch': 3.682572614107884}\n",
      "Step 3560/4820 ( 73.9%) │ Loss: 0.0278 │ LR: 8.73e-06\n",
      "{'loss': 0.0278, 'grad_norm': 0.47444596886634827, 'learning_rate': 8.727524204702628e-06, 'epoch': 3.6929460580912865}\n",
      "Step 3570/4820 ( 74.1%) │ Loss: 0.0282 │ LR: 8.66e-06\n",
      "{'loss': 0.0282, 'grad_norm': 0.7609511017799377, 'learning_rate': 8.658367911479943e-06, 'epoch': 3.703319502074689}\n",
      "Step 3580/4820 ( 74.3%) │ Loss: 0.0194 │ LR: 8.59e-06\n",
      "{'loss': 0.0194, 'grad_norm': 1.6732112169265747, 'learning_rate': 8.58921161825726e-06, 'epoch': 3.7136929460580914}\n",
      "Step 3590/4820 ( 74.5%) │ Loss: 0.0127 │ LR: 8.52e-06\n",
      "{'loss': 0.0127, 'grad_norm': 1.7046464681625366, 'learning_rate': 8.520055325034579e-06, 'epoch': 3.724066390041494}\n",
      "Step 3600/4820 ( 74.7%) │ Loss: 0.0469 │ LR: 8.45e-06\n",
      "{'loss': 0.0469, 'grad_norm': 1.853743553161621, 'learning_rate': 8.450899031811896e-06, 'epoch': 3.7344398340248963}\n",
      "Step 3610/4820 ( 74.9%) │ Loss: 0.0156 │ LR: 8.38e-06\n",
      "{'loss': 0.0156, 'grad_norm': 1.763330340385437, 'learning_rate': 8.381742738589213e-06, 'epoch': 3.7448132780082988}\n",
      "Step 3620/4820 ( 75.1%) │ Loss: 0.0120 │ LR: 8.31e-06\n",
      "{'loss': 0.012, 'grad_norm': 0.693260133266449, 'learning_rate': 8.31258644536653e-06, 'epoch': 3.7551867219917012}\n",
      "Step 3630/4820 ( 75.3%) │ Loss: 0.0227 │ LR: 8.24e-06\n",
      "{'loss': 0.0227, 'grad_norm': 0.8902955651283264, 'learning_rate': 8.243430152143846e-06, 'epoch': 3.7655601659751037}\n",
      "Step 3640/4820 ( 75.5%) │ Loss: 0.0124 │ LR: 8.17e-06\n",
      "{'loss': 0.0124, 'grad_norm': 0.043936021625995636, 'learning_rate': 8.174273858921162e-06, 'epoch': 3.775933609958506}\n",
      "Step 3650/4820 ( 75.7%) │ Loss: 0.0166 │ LR: 8.11e-06\n",
      "{'loss': 0.0166, 'grad_norm': 0.12116993963718414, 'learning_rate': 8.105117565698478e-06, 'epoch': 3.7863070539419086}\n",
      "Step 3660/4820 ( 75.9%) │ Loss: 0.0235 │ LR: 8.04e-06\n",
      "{'loss': 0.0235, 'grad_norm': 1.1679201126098633, 'learning_rate': 8.035961272475795e-06, 'epoch': 3.796680497925311}\n",
      "Step 3670/4820 ( 76.1%) │ Loss: 0.0415 │ LR: 7.97e-06\n",
      "{'loss': 0.0415, 'grad_norm': 2.63165545463562, 'learning_rate': 7.966804979253112e-06, 'epoch': 3.8070539419087135}\n",
      "Step 3680/4820 ( 76.3%) │ Loss: 0.0315 │ LR: 7.90e-06\n",
      "{'loss': 0.0315, 'grad_norm': 0.423841655254364, 'learning_rate': 7.897648686030429e-06, 'epoch': 3.817427385892116}\n",
      "Step 3690/4820 ( 76.6%) │ Loss: 0.0231 │ LR: 7.83e-06\n",
      "{'loss': 0.0231, 'grad_norm': 1.1494958400726318, 'learning_rate': 7.828492392807746e-06, 'epoch': 3.8278008298755184}\n",
      "Step 3700/4820 ( 76.8%) │ Loss: 0.0252 │ LR: 7.76e-06\n",
      "{'loss': 0.0252, 'grad_norm': 2.412403106689453, 'learning_rate': 7.759336099585063e-06, 'epoch': 3.8381742738589213}\n",
      "Step 3710/4820 ( 77.0%) │ Loss: 0.0236 │ LR: 7.69e-06\n",
      "{'loss': 0.0236, 'grad_norm': 1.478977084159851, 'learning_rate': 7.690179806362378e-06, 'epoch': 3.8485477178423237}\n",
      "Step 3720/4820 ( 77.2%) │ Loss: 0.0154 │ LR: 7.62e-06\n",
      "{'loss': 0.0154, 'grad_norm': 0.3272201120853424, 'learning_rate': 7.621023513139696e-06, 'epoch': 3.858921161825726}\n",
      "Step 3730/4820 ( 77.4%) │ Loss: 0.0158 │ LR: 7.55e-06\n",
      "{'loss': 0.0158, 'grad_norm': 0.5367488265037537, 'learning_rate': 7.5518672199170125e-06, 'epoch': 3.8692946058091287}\n",
      "Step 3740/4820 ( 77.6%) │ Loss: 0.0141 │ LR: 7.48e-06\n",
      "{'loss': 0.0141, 'grad_norm': 0.719971239566803, 'learning_rate': 7.482710926694329e-06, 'epoch': 3.879668049792531}\n",
      "Step 3750/4820 ( 77.8%) │ Loss: 0.0186 │ LR: 7.41e-06\n",
      "{'loss': 0.0186, 'grad_norm': 0.8157810568809509, 'learning_rate': 7.413554633471646e-06, 'epoch': 3.8900414937759336}\n",
      "Step 3760/4820 ( 78.0%) │ Loss: 0.0193 │ LR: 7.34e-06\n",
      "{'loss': 0.0193, 'grad_norm': 1.1409428119659424, 'learning_rate': 7.344398340248962e-06, 'epoch': 3.900414937759336}\n",
      "Step 3770/4820 ( 78.2%) │ Loss: 0.0359 │ LR: 7.28e-06\n",
      "{'loss': 0.0359, 'grad_norm': 0.6800336241722107, 'learning_rate': 7.275242047026279e-06, 'epoch': 3.9107883817427385}\n",
      "Step 3780/4820 ( 78.4%) │ Loss: 0.0473 │ LR: 7.21e-06\n",
      "{'loss': 0.0473, 'grad_norm': 0.148868590593338, 'learning_rate': 7.206085753803597e-06, 'epoch': 3.921161825726141}\n",
      "Step 3790/4820 ( 78.6%) │ Loss: 0.0153 │ LR: 7.14e-06\n",
      "{'loss': 0.0153, 'grad_norm': 0.7230252623558044, 'learning_rate': 7.136929460580913e-06, 'epoch': 3.931535269709544}\n",
      "Step 3800/4820 ( 78.8%) │ Loss: 0.0320 │ LR: 7.07e-06\n",
      "{'loss': 0.032, 'grad_norm': 2.106719970703125, 'learning_rate': 7.06777316735823e-06, 'epoch': 3.9419087136929463}\n",
      "Step 3810/4820 ( 79.0%) │ Loss: 0.0300 │ LR: 7.00e-06\n",
      "{'loss': 0.03, 'grad_norm': 1.117416262626648, 'learning_rate': 6.998616874135547e-06, 'epoch': 3.9522821576763487}\n",
      "Step 3820/4820 ( 79.3%) │ Loss: 0.0228 │ LR: 6.93e-06\n",
      "{'loss': 0.0228, 'grad_norm': 1.1070019006729126, 'learning_rate': 6.9294605809128635e-06, 'epoch': 3.962655601659751}\n",
      "Step 3830/4820 ( 79.5%) │ Loss: 0.0357 │ LR: 6.86e-06\n",
      "{'loss': 0.0357, 'grad_norm': 0.5674139261245728, 'learning_rate': 6.8603042876901796e-06, 'epoch': 3.9730290456431536}\n",
      "Step 3840/4820 ( 79.7%) │ Loss: 0.0537 │ LR: 6.79e-06\n",
      "{'loss': 0.0537, 'grad_norm': 0.8570712208747864, 'learning_rate': 6.7911479944674964e-06, 'epoch': 3.983402489626556}\n",
      "Step 3850/4820 ( 79.9%) │ Loss: 0.0106 │ LR: 6.72e-06\n",
      "{'loss': 0.0106, 'grad_norm': 1.148435115814209, 'learning_rate': 6.721991701244813e-06, 'epoch': 3.9937759336099585}\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Epoch completed in 719.5s\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Validation │ F1: 0.9816 │ Precision: 0.9848 │ Recall: 0.9785\n",
      "{'eval_loss': 0.034436892718076706, 'eval_precision': 0.9847596008466888, 'eval_recall': 0.9784868697794604, 'eval_f1': 0.981613214371835, 'eval_runtime': 63.2903, 'eval_samples_per_second': 64.149, 'eval_steps_per_second': 21.393, 'epoch': 4.0}\n",
      "\n",
      "Epoch 5/5\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Step 3860/4820 ( 80.1%) │ Loss: 0.0273 │ LR: 6.65e-06\n",
      "{'loss': 0.0273, 'grad_norm': 0.8822254538536072, 'learning_rate': 6.65283540802213e-06, 'epoch': 4.004149377593361}\n",
      "Step 3870/4820 ( 80.3%) │ Loss: 0.0138 │ LR: 6.58e-06\n",
      "{'loss': 0.0138, 'grad_norm': 0.9763423800468445, 'learning_rate': 6.583679114799447e-06, 'epoch': 4.014522821576763}\n",
      "Step 3880/4820 ( 80.5%) │ Loss: 0.0150 │ LR: 6.51e-06\n",
      "{'loss': 0.015, 'grad_norm': 0.5763760805130005, 'learning_rate': 6.514522821576764e-06, 'epoch': 4.024896265560166}\n",
      "Step 3890/4820 ( 80.7%) │ Loss: 0.0422 │ LR: 6.45e-06\n",
      "{'loss': 0.0422, 'grad_norm': 1.2469719648361206, 'learning_rate': 6.445366528354081e-06, 'epoch': 4.035269709543568}\n",
      "Step 3900/4820 ( 80.9%) │ Loss: 0.0079 │ LR: 6.38e-06\n",
      "{'loss': 0.0079, 'grad_norm': 1.503426432609558, 'learning_rate': 6.376210235131397e-06, 'epoch': 4.045643153526971}\n",
      "Step 3910/4820 ( 81.1%) │ Loss: 0.0152 │ LR: 6.31e-06\n",
      "{'loss': 0.0152, 'grad_norm': 1.1584326028823853, 'learning_rate': 6.307053941908714e-06, 'epoch': 4.056016597510373}\n",
      "Step 3920/4820 ( 81.3%) │ Loss: 0.0150 │ LR: 6.24e-06\n",
      "{'loss': 0.015, 'grad_norm': 0.5727489590644836, 'learning_rate': 6.237897648686031e-06, 'epoch': 4.066390041493776}\n",
      "Step 3930/4820 ( 81.5%) │ Loss: 0.0319 │ LR: 6.17e-06\n",
      "{'loss': 0.0319, 'grad_norm': 0.7292370200157166, 'learning_rate': 6.1687413554633475e-06, 'epoch': 4.076763485477178}\n",
      "Step 3940/4820 ( 81.7%) │ Loss: 0.0365 │ LR: 6.10e-06\n",
      "{'loss': 0.0365, 'grad_norm': 0.8250554800033569, 'learning_rate': 6.099585062240664e-06, 'epoch': 4.087136929460581}\n",
      "Step 3950/4820 ( 82.0%) │ Loss: 0.0111 │ LR: 6.03e-06\n",
      "{'loss': 0.0111, 'grad_norm': 0.4358675479888916, 'learning_rate': 6.030428769017981e-06, 'epoch': 4.097510373443983}\n",
      "Step 3960/4820 ( 82.2%) │ Loss: 0.0148 │ LR: 5.96e-06\n",
      "{'loss': 0.0148, 'grad_norm': 0.036828573793172836, 'learning_rate': 5.961272475795297e-06, 'epoch': 4.1078838174273855}\n",
      "Step 3970/4820 ( 82.4%) │ Loss: 0.0175 │ LR: 5.89e-06\n",
      "{'loss': 0.0175, 'grad_norm': 0.9539833664894104, 'learning_rate': 5.892116182572614e-06, 'epoch': 4.118257261410788}\n",
      "Step 3980/4820 ( 82.6%) │ Loss: 0.0055 │ LR: 5.82e-06\n",
      "{'loss': 0.0055, 'grad_norm': 0.6126735806465149, 'learning_rate': 5.822959889349931e-06, 'epoch': 4.12863070539419}\n",
      "Step 3990/4820 ( 82.8%) │ Loss: 0.0186 │ LR: 5.75e-06\n",
      "{'loss': 0.0186, 'grad_norm': 0.11471915245056152, 'learning_rate': 5.753803596127248e-06, 'epoch': 4.139004149377594}\n",
      "Step 4000/4820 ( 83.0%) │ Loss: 0.0222 │ LR: 5.68e-06\n",
      "{'loss': 0.0222, 'grad_norm': 0.2862769067287445, 'learning_rate': 5.684647302904564e-06, 'epoch': 4.149377593360996}\n",
      "Step 4010/4820 ( 83.2%) │ Loss: 0.0386 │ LR: 5.62e-06\n",
      "{'loss': 0.0386, 'grad_norm': 0.9437434673309326, 'learning_rate': 5.615491009681882e-06, 'epoch': 4.159751037344399}\n",
      "Step 4020/4820 ( 83.4%) │ Loss: 0.0084 │ LR: 5.55e-06\n",
      "{'loss': 0.0084, 'grad_norm': 0.21228499710559845, 'learning_rate': 5.5463347164591985e-06, 'epoch': 4.170124481327801}\n",
      "Step 4030/4820 ( 83.6%) │ Loss: 0.0251 │ LR: 5.48e-06\n",
      "{'loss': 0.0251, 'grad_norm': 2.0610804557800293, 'learning_rate': 5.4771784232365145e-06, 'epoch': 4.180497925311204}\n",
      "Step 4040/4820 ( 83.8%) │ Loss: 0.0205 │ LR: 5.41e-06\n",
      "{'loss': 0.0205, 'grad_norm': 1.0304272174835205, 'learning_rate': 5.408022130013831e-06, 'epoch': 4.190871369294606}\n",
      "Step 4050/4820 ( 84.0%) │ Loss: 0.0130 │ LR: 5.34e-06\n",
      "{'loss': 0.013, 'grad_norm': 0.2944679856300354, 'learning_rate': 5.338865836791148e-06, 'epoch': 4.2012448132780085}\n",
      "Step 4060/4820 ( 84.2%) │ Loss: 0.0113 │ LR: 5.27e-06\n",
      "{'loss': 0.0113, 'grad_norm': 0.27815690636634827, 'learning_rate': 5.269709543568464e-06, 'epoch': 4.211618257261411}\n",
      "Step 4070/4820 ( 84.4%) │ Loss: 0.0151 │ LR: 5.20e-06\n",
      "{'loss': 0.0151, 'grad_norm': 7.45960807800293, 'learning_rate': 5.200553250345781e-06, 'epoch': 4.221991701244813}\n",
      "Step 4080/4820 ( 84.6%) │ Loss: 0.0211 │ LR: 5.13e-06\n",
      "{'loss': 0.0211, 'grad_norm': 3.032463312149048, 'learning_rate': 5.131396957123098e-06, 'epoch': 4.232365145228216}\n",
      "Step 4090/4820 ( 84.9%) │ Loss: 0.0119 │ LR: 5.06e-06\n",
      "{'loss': 0.0119, 'grad_norm': 1.408074140548706, 'learning_rate': 5.062240663900416e-06, 'epoch': 4.242738589211618}\n",
      "Step 4100/4820 ( 85.1%) │ Loss: 0.0116 │ LR: 4.99e-06\n",
      "{'loss': 0.0116, 'grad_norm': 0.1890159398317337, 'learning_rate': 4.993084370677732e-06, 'epoch': 4.253112033195021}\n",
      "Step 4110/4820 ( 85.3%) │ Loss: 0.0121 │ LR: 4.92e-06\n",
      "{'loss': 0.0121, 'grad_norm': 0.5787109136581421, 'learning_rate': 4.923928077455049e-06, 'epoch': 4.263485477178423}\n",
      "Step 4120/4820 ( 85.5%) │ Loss: 0.0192 │ LR: 4.85e-06\n",
      "{'loss': 0.0192, 'grad_norm': 0.2988644242286682, 'learning_rate': 4.8547717842323655e-06, 'epoch': 4.273858921161826}\n",
      "Step 4130/4820 ( 85.7%) │ Loss: 0.0128 │ LR: 4.79e-06\n",
      "{'loss': 0.0128, 'grad_norm': 0.4383576512336731, 'learning_rate': 4.7856154910096815e-06, 'epoch': 4.284232365145228}\n",
      "Step 4140/4820 ( 85.9%) │ Loss: 0.0145 │ LR: 4.72e-06\n",
      "{'loss': 0.0145, 'grad_norm': 0.4204467236995697, 'learning_rate': 4.716459197786998e-06, 'epoch': 4.2946058091286305}\n",
      "Step 4150/4820 ( 86.1%) │ Loss: 0.0341 │ LR: 4.65e-06\n",
      "{'loss': 0.0341, 'grad_norm': 0.8318340182304382, 'learning_rate': 4.647302904564315e-06, 'epoch': 4.304979253112033}\n",
      "Step 4160/4820 ( 86.3%) │ Loss: 0.0148 │ LR: 4.58e-06\n",
      "{'loss': 0.0148, 'grad_norm': 0.14028161764144897, 'learning_rate': 4.578146611341633e-06, 'epoch': 4.3153526970954355}\n",
      "Step 4170/4820 ( 86.5%) │ Loss: 0.0147 │ LR: 4.51e-06\n",
      "{'loss': 0.0147, 'grad_norm': 0.2997358441352844, 'learning_rate': 4.508990318118949e-06, 'epoch': 4.325726141078838}\n",
      "Step 4180/4820 ( 86.7%) │ Loss: 0.0186 │ LR: 4.44e-06\n",
      "{'loss': 0.0186, 'grad_norm': 0.33671340346336365, 'learning_rate': 4.439834024896266e-06, 'epoch': 4.33609958506224}\n",
      "Step 4190/4820 ( 86.9%) │ Loss: 0.0281 │ LR: 4.37e-06\n",
      "{'loss': 0.0281, 'grad_norm': 0.18695661425590515, 'learning_rate': 4.370677731673583e-06, 'epoch': 4.346473029045643}\n",
      "Step 4200/4820 ( 87.1%) │ Loss: 0.0145 │ LR: 4.30e-06\n",
      "{'loss': 0.0145, 'grad_norm': 0.8589046001434326, 'learning_rate': 4.301521438450899e-06, 'epoch': 4.356846473029045}\n",
      "Step 4210/4820 ( 87.3%) │ Loss: 0.0192 │ LR: 4.23e-06\n",
      "{'loss': 0.0192, 'grad_norm': 1.374254584312439, 'learning_rate': 4.232365145228216e-06, 'epoch': 4.367219917012449}\n",
      "Step 4220/4820 ( 87.6%) │ Loss: 0.0356 │ LR: 4.16e-06\n",
      "{'loss': 0.0356, 'grad_norm': 0.3024667501449585, 'learning_rate': 4.1632088520055325e-06, 'epoch': 4.377593360995851}\n",
      "Step 4230/4820 ( 87.8%) │ Loss: 0.0252 │ LR: 4.09e-06\n",
      "{'loss': 0.0252, 'grad_norm': 1.7465691566467285, 'learning_rate': 4.0940525587828486e-06, 'epoch': 4.3879668049792535}\n",
      "Step 4240/4820 ( 88.0%) │ Loss: 0.0050 │ LR: 4.02e-06\n",
      "{'loss': 0.005, 'grad_norm': 0.30590519309043884, 'learning_rate': 4.024896265560166e-06, 'epoch': 4.398340248962656}\n",
      "Step 4250/4820 ( 88.2%) │ Loss: 0.0160 │ LR: 3.96e-06\n",
      "{'loss': 0.016, 'grad_norm': 0.7917174696922302, 'learning_rate': 3.955739972337483e-06, 'epoch': 4.408713692946058}\n",
      "Step 4260/4820 ( 88.4%) │ Loss: 0.0170 │ LR: 3.89e-06\n",
      "{'loss': 0.017, 'grad_norm': 0.8254898190498352, 'learning_rate': 3.8865836791148e-06, 'epoch': 4.419087136929461}\n",
      "Step 4270/4820 ( 88.6%) │ Loss: 0.0129 │ LR: 3.82e-06\n",
      "{'loss': 0.0129, 'grad_norm': 0.6531630754470825, 'learning_rate': 3.817427385892116e-06, 'epoch': 4.429460580912863}\n",
      "Step 4280/4820 ( 88.8%) │ Loss: 0.0098 │ LR: 3.75e-06\n",
      "{'loss': 0.0098, 'grad_norm': 0.023946262896060944, 'learning_rate': 3.748271092669433e-06, 'epoch': 4.439834024896266}\n",
      "Step 4290/4820 ( 89.0%) │ Loss: 0.0225 │ LR: 3.68e-06\n",
      "{'loss': 0.0225, 'grad_norm': 0.19636477530002594, 'learning_rate': 3.6791147994467494e-06, 'epoch': 4.450207468879668}\n",
      "Step 4300/4820 ( 89.2%) │ Loss: 0.0356 │ LR: 3.61e-06\n",
      "{'loss': 0.0356, 'grad_norm': 0.8397195339202881, 'learning_rate': 3.6099585062240667e-06, 'epoch': 4.460580912863071}\n",
      "Step 4310/4820 ( 89.4%) │ Loss: 0.0137 │ LR: 3.54e-06\n",
      "{'loss': 0.0137, 'grad_norm': 0.8765822052955627, 'learning_rate': 3.540802213001383e-06, 'epoch': 4.470954356846473}\n",
      "Step 4320/4820 ( 89.6%) │ Loss: 0.0361 │ LR: 3.47e-06\n",
      "{'loss': 0.0361, 'grad_norm': 0.6562119722366333, 'learning_rate': 3.4716459197787e-06, 'epoch': 4.481327800829876}\n",
      "Step 4330/4820 ( 89.8%) │ Loss: 0.0210 │ LR: 3.40e-06\n",
      "{'loss': 0.021, 'grad_norm': 0.46784159541130066, 'learning_rate': 3.402489626556017e-06, 'epoch': 4.491701244813278}\n",
      "Step 4340/4820 ( 90.0%) │ Loss: 0.0064 │ LR: 3.33e-06\n",
      "{'loss': 0.0064, 'grad_norm': 0.06563392281532288, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.5020746887966805}\n",
      "Step 4350/4820 ( 90.2%) │ Loss: 0.0078 │ LR: 3.26e-06\n",
      "{'loss': 0.0078, 'grad_norm': 1.8041291236877441, 'learning_rate': 3.26417704011065e-06, 'epoch': 4.512448132780083}\n",
      "Step 4360/4820 ( 90.5%) │ Loss: 0.0123 │ LR: 3.20e-06\n",
      "{'loss': 0.0123, 'grad_norm': 1.1327776908874512, 'learning_rate': 3.1950207468879666e-06, 'epoch': 4.522821576763485}\n",
      "Step 4370/4820 ( 90.7%) │ Loss: 0.0183 │ LR: 3.13e-06\n",
      "{'loss': 0.0183, 'grad_norm': 0.0450407974421978, 'learning_rate': 3.125864453665284e-06, 'epoch': 4.533195020746888}\n",
      "Step 4380/4820 ( 90.9%) │ Loss: 0.0302 │ LR: 3.06e-06\n",
      "{'loss': 0.0302, 'grad_norm': 2.0724432468414307, 'learning_rate': 3.0567081604426004e-06, 'epoch': 4.54356846473029}\n",
      "Step 4390/4820 ( 91.1%) │ Loss: 0.0352 │ LR: 2.99e-06\n",
      "{'loss': 0.0352, 'grad_norm': 0.24159061908721924, 'learning_rate': 2.987551867219917e-06, 'epoch': 4.553941908713693}\n",
      "Step 4400/4820 ( 91.3%) │ Loss: 0.0184 │ LR: 2.92e-06\n",
      "{'loss': 0.0184, 'grad_norm': 0.37505650520324707, 'learning_rate': 2.9183955739972337e-06, 'epoch': 4.564315352697095}\n",
      "Step 4410/4820 ( 91.5%) │ Loss: 0.0214 │ LR: 2.85e-06\n",
      "{'loss': 0.0214, 'grad_norm': 0.5753120183944702, 'learning_rate': 2.8492392807745506e-06, 'epoch': 4.574688796680498}\n",
      "Step 4420/4820 ( 91.7%) │ Loss: 0.0302 │ LR: 2.78e-06\n",
      "{'loss': 0.0302, 'grad_norm': 1.025758147239685, 'learning_rate': 2.7800829875518675e-06, 'epoch': 4.5850622406639}\n",
      "Step 4430/4820 ( 91.9%) │ Loss: 0.0120 │ LR: 2.71e-06\n",
      "{'loss': 0.012, 'grad_norm': 0.027648862451314926, 'learning_rate': 2.710926694329184e-06, 'epoch': 4.595435684647303}\n",
      "Step 4440/4820 ( 92.1%) │ Loss: 0.0170 │ LR: 2.64e-06\n",
      "{'loss': 0.017, 'grad_norm': 0.5822761058807373, 'learning_rate': 2.6417704011065008e-06, 'epoch': 4.605809128630705}\n",
      "Step 4450/4820 ( 92.3%) │ Loss: 0.0054 │ LR: 2.57e-06\n",
      "{'loss': 0.0054, 'grad_norm': 0.9954983592033386, 'learning_rate': 2.5726141078838177e-06, 'epoch': 4.6161825726141075}\n",
      "Step 4460/4820 ( 92.5%) │ Loss: 0.0240 │ LR: 2.50e-06\n",
      "{'loss': 0.024, 'grad_norm': 0.21083305776119232, 'learning_rate': 2.503457814661134e-06, 'epoch': 4.62655601659751}\n",
      "Step 4470/4820 ( 92.7%) │ Loss: 0.0334 │ LR: 2.43e-06\n",
      "{'loss': 0.0334, 'grad_norm': 0.8658362627029419, 'learning_rate': 2.434301521438451e-06, 'epoch': 4.636929460580912}\n",
      "Step 4480/4820 ( 92.9%) │ Loss: 0.0180 │ LR: 2.37e-06\n",
      "{'loss': 0.018, 'grad_norm': 0.6270092725753784, 'learning_rate': 2.3651452282157674e-06, 'epoch': 4.647302904564316}\n",
      "Step 4490/4820 ( 93.2%) │ Loss: 0.0200 │ LR: 2.30e-06\n",
      "{'loss': 0.02, 'grad_norm': 1.0085456371307373, 'learning_rate': 2.2959889349930847e-06, 'epoch': 4.657676348547718}\n",
      "Step 4500/4820 ( 93.4%) │ Loss: 0.0137 │ LR: 2.23e-06\n",
      "{'loss': 0.0137, 'grad_norm': 0.7107728719711304, 'learning_rate': 2.226832641770401e-06, 'epoch': 4.668049792531121}\n",
      "Step 4510/4820 ( 93.6%) │ Loss: 0.0113 │ LR: 2.16e-06\n",
      "{'loss': 0.0113, 'grad_norm': 0.3514105975627899, 'learning_rate': 2.1576763485477176e-06, 'epoch': 4.678423236514523}\n",
      "Step 4520/4820 ( 93.8%) │ Loss: 0.0119 │ LR: 2.09e-06\n",
      "{'loss': 0.0119, 'grad_norm': 0.8756006956100464, 'learning_rate': 2.0885200553250345e-06, 'epoch': 4.6887966804979255}\n",
      "Step 4530/4820 ( 94.0%) │ Loss: 0.0149 │ LR: 2.02e-06\n",
      "{'loss': 0.0149, 'grad_norm': 0.21255792677402496, 'learning_rate': 2.0193637621023514e-06, 'epoch': 4.699170124481328}\n",
      "Step 4540/4820 ( 94.2%) │ Loss: 0.0174 │ LR: 1.95e-06\n",
      "{'loss': 0.0174, 'grad_norm': 0.2691637873649597, 'learning_rate': 1.9502074688796682e-06, 'epoch': 4.70954356846473}\n",
      "Step 4550/4820 ( 94.4%) │ Loss: 0.0167 │ LR: 1.88e-06\n",
      "{'loss': 0.0167, 'grad_norm': 3.9570155143737793, 'learning_rate': 1.881051175656985e-06, 'epoch': 4.719917012448133}\n",
      "Step 4560/4820 ( 94.6%) │ Loss: 0.0229 │ LR: 1.81e-06\n",
      "{'loss': 0.0229, 'grad_norm': 0.9121626615524292, 'learning_rate': 1.8118948824343016e-06, 'epoch': 4.730290456431535}\n",
      "Step 4570/4820 ( 94.8%) │ Loss: 0.0163 │ LR: 1.74e-06\n",
      "{'loss': 0.0163, 'grad_norm': 0.9855076670646667, 'learning_rate': 1.7427385892116184e-06, 'epoch': 4.740663900414938}\n",
      "Step 4580/4820 ( 95.0%) │ Loss: 0.0232 │ LR: 1.67e-06\n",
      "{'loss': 0.0232, 'grad_norm': 0.7918475866317749, 'learning_rate': 1.673582295988935e-06, 'epoch': 4.75103734439834}\n",
      "Step 4590/4820 ( 95.2%) │ Loss: 0.0131 │ LR: 1.60e-06\n",
      "{'loss': 0.0131, 'grad_norm': 0.870931088924408, 'learning_rate': 1.6044260027662518e-06, 'epoch': 4.761410788381743}\n",
      "Step 4600/4820 ( 95.4%) │ Loss: 0.0054 │ LR: 1.54e-06\n",
      "{'loss': 0.0054, 'grad_norm': 0.2252039760351181, 'learning_rate': 1.5352697095435684e-06, 'epoch': 4.771784232365145}\n",
      "Step 4610/4820 ( 95.6%) │ Loss: 0.0048 │ LR: 1.47e-06\n",
      "{'loss': 0.0048, 'grad_norm': 0.11892292648553848, 'learning_rate': 1.4661134163208853e-06, 'epoch': 4.782157676348548}\n",
      "Step 4620/4820 ( 95.9%) │ Loss: 0.0126 │ LR: 1.40e-06\n",
      "{'loss': 0.0126, 'grad_norm': 0.632530689239502, 'learning_rate': 1.396957123098202e-06, 'epoch': 4.79253112033195}\n",
      "Step 4630/4820 ( 96.1%) │ Loss: 0.0126 │ LR: 1.33e-06\n",
      "{'loss': 0.0126, 'grad_norm': 0.03741902858018875, 'learning_rate': 1.3278008298755188e-06, 'epoch': 4.8029045643153525}\n",
      "Step 4640/4820 ( 96.3%) │ Loss: 0.0098 │ LR: 1.26e-06\n",
      "{'loss': 0.0098, 'grad_norm': 2.53582501411438, 'learning_rate': 1.2586445366528353e-06, 'epoch': 4.813278008298755}\n",
      "Step 4650/4820 ( 96.5%) │ Loss: 0.0149 │ LR: 1.19e-06\n",
      "{'loss': 0.0149, 'grad_norm': 0.30941513180732727, 'learning_rate': 1.1894882434301522e-06, 'epoch': 4.823651452282157}\n",
      "Step 4660/4820 ( 96.7%) │ Loss: 0.0096 │ LR: 1.12e-06\n",
      "{'loss': 0.0096, 'grad_norm': 0.017513001337647438, 'learning_rate': 1.1203319502074688e-06, 'epoch': 4.83402489626556}\n",
      "Step 4670/4820 ( 96.9%) │ Loss: 0.0173 │ LR: 1.05e-06\n",
      "{'loss': 0.0173, 'grad_norm': 1.0772473812103271, 'learning_rate': 1.0511756569847857e-06, 'epoch': 4.844398340248962}\n",
      "Step 4680/4820 ( 97.1%) │ Loss: 0.0182 │ LR: 9.82e-07\n",
      "{'loss': 0.0182, 'grad_norm': 0.24438101053237915, 'learning_rate': 9.820193637621024e-07, 'epoch': 4.854771784232365}\n",
      "Step 4690/4820 ( 97.3%) │ Loss: 0.0174 │ LR: 9.13e-07\n",
      "{'loss': 0.0174, 'grad_norm': 0.17818020284175873, 'learning_rate': 9.128630705394191e-07, 'epoch': 4.865145228215768}\n",
      "Step 4700/4820 ( 97.5%) │ Loss: 0.0220 │ LR: 8.44e-07\n",
      "{'loss': 0.022, 'grad_norm': 0.6740302443504333, 'learning_rate': 8.437067773167359e-07, 'epoch': 4.875518672199171}\n",
      "Step 4710/4820 ( 97.7%) │ Loss: 0.0194 │ LR: 7.75e-07\n",
      "{'loss': 0.0194, 'grad_norm': 0.565890371799469, 'learning_rate': 7.745504840940525e-07, 'epoch': 4.885892116182573}\n",
      "Step 4720/4820 ( 97.9%) │ Loss: 0.0244 │ LR: 7.05e-07\n",
      "{'loss': 0.0244, 'grad_norm': 1.7097270488739014, 'learning_rate': 7.053941908713693e-07, 'epoch': 4.8962655601659755}\n",
      "Step 4730/4820 ( 98.1%) │ Loss: 0.0126 │ LR: 6.36e-07\n",
      "{'loss': 0.0126, 'grad_norm': 0.3674108386039734, 'learning_rate': 6.362378976486861e-07, 'epoch': 4.906639004149378}\n",
      "Step 4740/4820 ( 98.3%) │ Loss: 0.0168 │ LR: 5.67e-07\n",
      "{'loss': 0.0168, 'grad_norm': 0.5319406390190125, 'learning_rate': 5.670816044260027e-07, 'epoch': 4.91701244813278}\n",
      "Step 4750/4820 ( 98.5%) │ Loss: 0.0158 │ LR: 4.98e-07\n",
      "{'loss': 0.0158, 'grad_norm': 0.3305719196796417, 'learning_rate': 4.979253112033195e-07, 'epoch': 4.927385892116183}\n",
      "Step 4760/4820 ( 98.8%) │ Loss: 0.0162 │ LR: 4.29e-07\n",
      "{'loss': 0.0162, 'grad_norm': 0.47981780767440796, 'learning_rate': 4.287690179806362e-07, 'epoch': 4.937759336099585}\n",
      "Step 4770/4820 ( 99.0%) │ Loss: 0.0275 │ LR: 3.60e-07\n",
      "{'loss': 0.0275, 'grad_norm': 0.111110158264637, 'learning_rate': 3.59612724757953e-07, 'epoch': 4.948132780082988}\n",
      "Step 4780/4820 ( 99.2%) │ Loss: 0.0325 │ LR: 2.90e-07\n",
      "{'loss': 0.0325, 'grad_norm': 0.8626731634140015, 'learning_rate': 2.904564315352697e-07, 'epoch': 4.95850622406639}\n",
      "Step 4790/4820 ( 99.4%) │ Loss: 0.0186 │ LR: 2.21e-07\n",
      "{'loss': 0.0186, 'grad_norm': 0.8591527342796326, 'learning_rate': 2.2130013831258645e-07, 'epoch': 4.968879668049793}\n",
      "Step 4800/4820 ( 99.6%) │ Loss: 0.0375 │ LR: 1.52e-07\n",
      "{'loss': 0.0375, 'grad_norm': 0.5618607401847839, 'learning_rate': 1.5214384508990319e-07, 'epoch': 4.979253112033195}\n",
      "Step 4810/4820 ( 99.8%) │ Loss: 0.0447 │ LR: 8.30e-08\n",
      "{'loss': 0.0447, 'grad_norm': 1.6724265813827515, 'learning_rate': 8.298755186721993e-08, 'epoch': 4.9896265560165975}\n",
      "Step 4820/4820 (100.0%) │ Loss: 0.0226 │ LR: 1.38e-08\n",
      "{'loss': 0.0226, 'grad_norm': 0.11070940643548965, 'learning_rate': 1.3831258644536653e-08, 'epoch': 5.0}\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "✓ Epoch completed in 741.4s\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Validation │ F1: 0.9822 │ Precision: 0.9848 │ Recall: 0.9796\n",
      "{'eval_loss': 0.03478514030575752, 'eval_precision': 0.9848365855132, 'eval_recall': 0.9796286280872544, 'eval_f1': 0.9822257034403807, 'eval_runtime': 78.2724, 'eval_samples_per_second': 51.87, 'eval_steps_per_second': 17.299, 'epoch': 5.0}\n",
      "{'train_runtime': 4232.7551, 'train_samples_per_second': 22.771, 'train_steps_per_second': 1.139, 'train_loss': 0.17407219163238755, 'epoch': 5.0}\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "Total time: 70.5 minutes (4233s)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"STARTING TRAINING\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=3,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=gradient_accumulation,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,  \n",
    "    logging_strategy='steps',\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    seed=42,\n",
    "    report_to='none',\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "class Every10StepsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time()\n",
    "        total_steps = state.max_steps\n",
    "        print(f\"Training for {args.num_train_epochs} epochs ({total_steps} total steps)\")\n",
    "\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start_time = time()\n",
    "        epoch_num = int(state.epoch) + 1\n",
    "        print(f\"\\nEpoch {epoch_num}/{args.num_train_epochs}\")\n",
    "        print(\"─\"*80)\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and state.global_step > 0:\n",
    "            step = state.global_step\n",
    "            max_step = state.max_steps\n",
    "            progress = (step / max_step) * 100\n",
    "            \n",
    "         \n",
    "            if 'loss' in logs:\n",
    "                msg = f\"Step {step:4d}/{max_step} ({progress:5.1f}%)\"\n",
    "                msg += f\" │ Loss: {logs['loss']:.4f}\"\n",
    "                \n",
    "                if 'learning_rate' in logs:\n",
    "                    msg += f\" │ LR: {logs['learning_rate']:.2e}\"\n",
    "                \n",
    "                print(msg)\n",
    "            \n",
    "          \n",
    "            elif 'eval_f1' in logs:\n",
    "                print(f\"─\"*80)\n",
    "                print(f\"Validation │ F1: {logs['eval_f1']:.4f} │ \"\n",
    "                      f\"Precision: {logs['eval_precision']:.4f} │ \"\n",
    "                      f\"Recall: {logs['eval_recall']:.4f}\")\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_start_time:\n",
    "            epoch_time = time() - self.epoch_start_time\n",
    "            print(f\"─\"*80)\n",
    "            print(f\"✓ Epoch completed in {epoch_time:.1f}s\\n\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.start_time:\n",
    "            total_time = time() - self.start_time\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"TRAINING COMPLETE\")\n",
    "            print(f\"Total time: {total_time/60:.1f} minutes ({total_time:.0f}s)\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        Every10StepsCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db1c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_ner_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
