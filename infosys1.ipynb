{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Setup - Import Libraries\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings (optional, but makes output cleaner)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "print(\"pandas, matplotlib, and seaborn are ready.\")"
      ],
      "metadata": {
        "id": "o0nsoRGbncJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NFKlzcEZo4nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Milestone 1.1 - Data Collection (Load Data) [Robust Version]\n",
        "# ==============================================================================\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "print(\"--- IMPORTANT INSTRUCTIONS ---\")\n",
        "print(\"1. Find the 'archive.zip' file on your computer and unzip it.\")\n",
        "print(\"2. Click the 'Choose Files' button below.\")\n",
        "print(\"3. Select the 'preprocessed_content.csv' file (NOT the .zip file).\")\n",
        "print(\"4. Wait for the upload to complete.\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# This will open the file upload dialog\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the filename (it's the first key in the 'uploaded' dictionary)\n",
        "    file_name = next(iter(uploaded))\n",
        "\n",
        "    print(f\"\\nSuccessfully uploaded '{file_name}'\")\n",
        "\n",
        "    if file_name.endswith('.zip'):\n",
        "        print(\"\\n!!! ERROR: You uploaded a .zip file. Please re-run this cell and upload the 'preprocessed_content.csv' file.\")\n",
        "        raise Exception(\"Incorrect file type uploaded.\")\n",
        "\n",
        "    print(\"\\nLoading data using the robust 'python' engine...\")\n",
        "    print(\"(This may take a moment longer.)\")\n",
        "\n",
        "    # --- ROBUST FIXES ---\n",
        "    # We are using all fixes to handle this complex file:\n",
        "    # 1. encoding='latin1': Fixes 'utf-8' codec errors.\n",
        "    # 2. engine='python':     Fixes 'Buffer overflow' errors.\n",
        "    # 3. on_bad_lines='skip': Skips any other broken rows.\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        io.BytesIO(uploaded[file_name]),\n",
        "        encoding='latin1',\n",
        "        on_bad_lines='skip',\n",
        "        engine='python'\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"\\n!!! ERROR: The loaded DataFrame is empty.\")\n",
        "        print(\"This may be because you uploaded the .zip file. Please re-run and upload the .csv file.\")\n",
        "    else:\n",
        "        print(f\"--- Successfully loaded data into DataFrame ---\")\n",
        "        print(\"\\n--- Data Head (First 5 Rows) ---\")\n",
        "        print(df.head())\n",
        "        print(\"\\n--- Data Info (Columns & Data Types) ---\")\n",
        "        df.info()\n",
        "\n",
        "except Exception as e:\n",
        "    # This will catch the 'KeyboardInterrupt' if it happens again\n",
        "    if \"KeyboardInterrupt\" in str(e):\n",
        "        print(\"\\nUpload was canceled. Please re-run the cell and let the upload finish.\")\n",
        "    else:\n",
        "        print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "RyxmZymwoWLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Milestone 1.2 - Preprocessing Pipeline\n",
        "# ==============================================================================\n",
        "# Check if the dataframe 'df' was loaded successfully from Cell 2\n",
        "if 'df' in locals() and not df.empty:\n",
        "    print(\"--- Starting preprocessing pipeline ---\")\n",
        "\n",
        "    # --- Fix Index Column ---\n",
        "    # Check if the first column was loaded as 'Unnamed: 0'\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        print(\"Found 'Unnamed: 0' column, setting it as the index.\")\n",
        "        df = df.set_index('Unnamed: 0')\n",
        "\n",
        "    # Define the columns we are most interested in for the project\n",
        "    key_columns = ['ticker', 'year', 'preprocessed_content', 'ner_entities', 'e_score', 's_score', 'g_score']\n",
        "\n",
        "    # Check for missing values in these specific columns\n",
        "    print(\"\\nMissing Values (Before Cleaning):\")\n",
        "    print(df[key_columns].isnull().sum())\n",
        "\n",
        "    # For this milestone, we will focus on the scores.\n",
        "    # Let's drop any rows where the scores are missing.\n",
        "    df_cleaned = df.dropna(subset=['e_score', 's_score', 'g_score'])\n",
        "\n",
        "    print(f\"\\nOriginal entity count: {len(df)}\")\n",
        "    print(f\"Entity count after dropping rows with missing scores: {len(df_cleaned)}\")\n",
        "\n",
        "    print(\"\\n--- Preprocessing Complete ---\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found or is empty.\")\n",
        "    print(\"Please run Cell 2 successfully before running this cell.\")"
      ],
      "metadata": {
        "id": "9XuW1KVYxmvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 4: Milestone 1.3 - Exploratory Data Analysis (EDA)\n",
        "# ==============================================================================\n",
        "# Check if the cleaned dataframe 'df_cleaned' exists from Cell 3\n",
        "if 'df_cleaned' in locals():\n",
        "    print(\"--- Starting Exploratory Data Analysis ---\")\n",
        "    print(\"Generating plots for ESG Score distributions...\")\n",
        "\n",
        "    # Set the plot style\n",
        "    sns.set_style(\"whitegrid\")\n",
        "\n",
        "    # Create a figure with 3 subplots (one for each score)\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
        "    fig.suptitle('Milestone 1 EDA: Distribution of Financial Entities by ESG Scores', fontsize=16, y=1.02)\n",
        "\n",
        "    # --- Plot 1: Environmental (E) Score Distribution ---\n",
        "    sns.histplot(df_cleaned['e_score'], bins=30, kde=True, ax=axes[0], color='green')\n",
        "    axes[0].set_title('Distribution of Environmental (E) Scores')\n",
        "    axes[0].set_xlabel('E Score')\n",
        "    axes[0].set_ylabel('Number of Companies')\n",
        "\n",
        "    # --- Plot 2: Social (S) Score Distribution ---\n",
        "    sns.histplot(df_cleaned['s_score'], bins=30, kde=True, ax=axes[1], color='blue')\n",
        "    axes[1].set_title('Distribution of Social (S) Scores')\n",
        "    axes[1].set_xlabel('S Score')\n",
        "    axes[1].set_ylabel('Number of Companies')\n",
        "\n",
        "    # --- Plot 3: Governance (G) Score Distribution ---\n",
        "    sns.histplot(df_cleaned['g_score'], bins=30, kde=True, ax=axes[2], color='red')\n",
        "    axes[2].set_title('Distribution of Governance (G) Scores')\n",
        "    axes[2].set_xlabel('G Score')\n",
        "    axes[0].set_ylabel('Number of Companies')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the plots\n",
        "    print(\"\\nPlots generated successfully.\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Cleaned dataframe 'df_cleaned' not found.\")\n",
        "    print(\"Please run Cell 3 successfully before running this cell.\")"
      ],
      "metadata": {
        "id": "kp9LAgbBxqDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**milestone2**"
      ],
      "metadata": {
        "id": "MRpsZez_xtGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Setup - Install and Import Libraries for Milestone 2\n",
        "# ==============================================================================\n",
        "# Install spaCy and spaCy Transformers\n",
        "!pip install -q spacy-transformers\n",
        "!pip install -q spacy\n",
        "\n",
        "# Download a pre-trained transformer pipeline\n",
        "# This is our \"selected model\" (similar to BERT)\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import DocBin, Doc\n",
        "from spacy.training.example import Example\n",
        "import re\n",
        "import ast  # This is for safely evaluating the string-list in 'ner_entities'\n",
        "import random\n",
        "from spacy.cli.train import train\n",
        "from spacy.scorer import Scorer\n",
        "import json\n",
        "\n",
        "print(\"\\n--- spaCy and Transformers are ready ---\")\n",
        "print(\"Loaded 'en_core_web_trf' as our base model.\")"
      ],
      "metadata": {
        "id": "cU00j8QgxssP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Import All Libraries\n",
        "# ==============================================================================\n",
        "# For M1\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# For M2\n",
        "import spacy\n",
        "from spacy.tokens import DocBin, Doc\n",
        "from spacy.training.example import Example\n",
        "import re\n",
        "import ast  # This is for safely evaluating the string-list in 'ner_entities'\n",
        "import random\n",
        "from spacy.cli.train import train\n",
        "from spacy.scorer import Scorer\n",
        "import json\n",
        "\n",
        "# For File Upload\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "vOzXnbKjzX96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Load Data (Robust Version)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- IMPORTANT INSTRUCTIONS ---\")\n",
        "print(\"1. Unzip the 'archive.zip' file on your computer first.\")\n",
        "print(\"2. Click the 'Choose Files' button below.\")\n",
        "print(\"3. Select the 'preprocessed_content.csv' file (NOT the .zip file).\")\n",
        "print(\"4. Wait for the upload to complete.\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(f\"\\nSuccessfully uploaded '{file_name}'\")\n",
        "\n",
        "    print(\"\\nLoading data using the robust 'python' engine...\")\n",
        "\n",
        "    # Use all fixes: 'latin1' encoding, 'python' engine, and skip bad lines\n",
        "    df = pd.read_csv(\n",
        "        io.BytesIO(uploaded[file_name]),\n",
        "        encoding='latin1',\n",
        "        on_bad_lines='skip',\n",
        "        engine='python'\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"\\n!!! ERROR: The loaded DataFrame is empty.\")\n",
        "    else:\n",
        "        print(f\"--- Successfully loaded data into DataFrame ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "iVgLqZ0n2mbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 4: Preprocessing Pipeline (Milestone 1)\n",
        "# ==============================================================================\n",
        "if 'df' in locals() and not df.empty:\n",
        "    print(\"--- Starting preprocessing pipeline ---\")\n",
        "\n",
        "    # Fix Index Column\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        print(\"Found 'Unnamed: 0' column, setting it as the index.\")\n",
        "        df = df.set_index('Unnamed: 0')\n",
        "\n",
        "    print(f\"Original entity count: {len(df)}\")\n",
        "    print(\"\\n--- Data is clean and ready. ---\")\n",
        "\n",
        "    # Display basic info\n",
        "    df.info()\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found or is empty.\")\n",
        "    print(\"Please run Cell 3 successfully before running this cell.\")"
      ],
      "metadata": {
        "id": "y5k4kgtn4Zmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 5: Milestone 2.1 - Preprocessing Effectiveness\n",
        "# ==============================================================================\n",
        "if 'df' in locals():\n",
        "    print(\"--- Demonstrating Preprocessing (Tokenization & Lemmatization) ---\")\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_trf\")\n",
        "    # Get a text sample and ensure it's a string\n",
        "    sample_text = str(df['preprocessed_content'].dropna().iloc[0])\n",
        "    sample_text_short = sample_text[:250]\n",
        "\n",
        "    print(f\"Processing sample text:\\n'{sample_text_short}...'\")\n",
        "    doc = nlp(sample_text_short)\n",
        "\n",
        "    print(\"\\n--- Analysis Results ---\")\n",
        "    print(f\"{'Token':<15} | {'Lemma (Base Form)':<18} | {'Part of Speech':<15}\")\n",
        "    print(\"-\" * 52)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<15} | {token.lemma_:<18} | {token.pos_:<15}\")\n",
        "\n",
        "    print(\"\\n--- Preprocessing Demo Complete ---\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found. Please re-run Cell 3.\")"
      ],
      "metadata": {
        "id": "TN3Oh7LQ4b5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 6: Prepare Data for Fine-Tuning\n",
        "# ==============================================================================\n",
        "if 'df' in locals():\n",
        "    print(\"--- Preparing Data for Training ---\")\n",
        "\n",
        "    df_train = df.dropna(subset=['preprocessed_content', 'ner_entities'])\n",
        "    df_train = df_train[df_train['preprocessed_content'].str.len() > 50]\n",
        "    print(f\"Cleaned data count: {len(df_train)}\")\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for _, row in df_train.iterrows():\n",
        "        text = str(row['preprocessed_content'])\n",
        "        try:\n",
        "            entities_list = ast.literal_eval(row['ner_entities'])\n",
        "        except (ValueError, SyntaxError):\n",
        "            continue\n",
        "\n",
        "        spans = []\n",
        "        for ent_string in set(entities_list):\n",
        "            try:\n",
        "                ent_string_safe = str(ent_string)\n",
        "                for match in re.finditer(re.escape(ent_string_safe), text, flags=re.IGNORECASE):\n",
        "                    start, end = match.span()\n",
        "                    span = (start, end, \"FIN_ENTITY\")\n",
        "                    spans.append(span)\n",
        "            except re.error:\n",
        "                continue\n",
        "        if spans:\n",
        "            training_data.append((text, {\"entities\": spans}))\n",
        "\n",
        "    print(f\"\\nSuccessfully created {len(training_data)} labeled training examples.\")\n",
        "    if training_data:\n",
        "        print(\"\\n--- Example of a Training Item ---\")\n",
        "        print(training_data[0])\n",
        "    else:\n",
        "        print(\"\\n!!! WARNING: No training data was created. Check 'ner_entities' column.\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found. Please re-run Cell 3.\")"
      ],
      "metadata": {
        "id": "-qw-7N8Q4k9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 7: Create DocBin Files (Train/Validation Split)\n",
        "# ==============================================================================\n",
        "if 'training_data' in locals() and 'nlp' in locals():\n",
        "    print(\"--- Creating DocBin files for training and validation ---\")\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "    split_point = int(len(training_data) * 0.8)\n",
        "    train_data = training_data[:split_point]\n",
        "    test_data = training_data[split_point:]\n",
        "\n",
        "    print(f\"Total examples: {len(training_data)}\")\n",
        "    print(f\"Training examples: {len(train_data)}\")\n",
        "    print(f\"Validation examples: {len(test_data)}\")\n",
        "\n",
        "    # --- Create train.spacy ---\n",
        "    db_train = DocBin()\n",
        "    for text, annotations in train_data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        for start, end, label in annotations[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label)\n",
        "            if span is not None:\n",
        "                ents.append(span)\n",
        "        doc.ents = spacy.util.filter_spans(ents)\n",
        "        db_train.add(doc)\n",
        "    db_train.to_disk(\"./train.spacy\")\n",
        "\n",
        "    # --- Create dev.spacy (for validation) ---\n",
        "    db_test = DocBin()\n",
        "    for text, annotations in test_data:\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        for start, end, label in annotations[\"entities\"]:\n",
        "            span = doc.char_span(start, end, label=label)\n",
        "            if span is not None:\n",
        "                ents.append(span)\n",
        "        doc.ents = spacy.util.filter_spans(ents)\n",
        "        db_test.add(doc)\n",
        "    db_test.to_disk(\"./dev.spacy\")\n",
        "\n",
        "    print(\"\\n--- train.spacy and dev.spacy files created successfully! ---\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: 'training_data' or 'nlp' model not found.\")"
      ],
      "metadata": {
        "id": "V_M510Sk4vic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 8: Create a Labeled Base Model\n",
        "# ==============================================================================\n",
        "# This cell assumes 'nlp' (the transformer model) was loaded in Cell 5\n",
        "\n",
        "if 'nlp' in locals():\n",
        "    print(\"--- Creating a new base model with 'FIN_ENTITY' label ---\")\n",
        "\n",
        "    # Get the 'ner' component from the pipeline\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        print(\"!!! Error: 'ner' pipe not found in model.\")\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "        # Add your new custom label to the NER component\n",
        "        ner.add_label(\"FIN_ENTITY\")\n",
        "\n",
        "        # Save this modified model to a new directory\n",
        "        output_dir = \"./base_model_with_labels\"\n",
        "        nlp.to_disk(output_dir)\n",
        "\n",
        "        print(f\"\\nSuccessfully saved new base model (with label) to '{output_dir}'\")\n",
        "        print(\"This model is now ready to be fine-tuned.\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: 'nlp' model not loaded. Please re-run Cell 5.\")"
      ],
      "metadata": {
        "id": "Zmegc9Fa5Yrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 9: Create Training Config File [CORRECTED]\n",
        "# ==============================================================================\n",
        "# We use 'init fill-config' to create a config from our new base model.\n",
        "# THE FIX: We point to the 'config.cfg' FILE inside the directory,\n",
        "# not just the directory itself.\n",
        "\n",
        "!python -m spacy init fill-config ./base_model_with_labels/config.cfg ./config.cfg\n",
        "\n",
        "print(\"\\n--- config.cfg file created successfully! ---\")\n",
        "print(\"This config is now perfectly matched to your model.\")"
      ],
      "metadata": {
        "id": "3aSXoHsh5kXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 10 (NEW): Install Missing Lookups Data\n",
        "# ==============================================================================\n",
        "# This command installs the data package needed for 'lexeme_norm'\n",
        "!pip install -q spacy-lookups-data\n",
        "\n",
        "print(\"\\n--- spacy-lookups-data installed successfully! ---\")"
      ],
      "metadata": {
        "id": "zeTwDjO-6Wiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 11: Milestone 2.2 - Model Training (Fine-Tuning)\n",
        "# ==============================================================================\n",
        "print(\"--- Starting Model Training ---\")\n",
        "print(\"This will take 5-10 minutes. Please wait...\")\n",
        "\n",
        "# We just provide the config file and our data paths.\n",
        "# No overrides are needed.\n",
        "# To experiment with hyperparameters, you can change 'max_epochs 5'\n",
        "!python -m spacy train ./config.cfg --output ./output_model \\\n",
        "--paths.train ./train.spacy \\\n",
        "--paths.dev ./dev.spacy \\\n",
        "--training.max_epochs 5\n",
        "\n",
        "print(\"\\n--- Training Complete! ---\")\n",
        "print(\"Your fine-tuned model is saved in 'output_model/model-best'.\")"
      ],
      "metadata": {
        "id": "m5_VwlSC8Z4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new\n"
      ],
      "metadata": {
        "id": "LbbOTG3u9Cfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Install Libraries (Simplified & Corrected)\n",
        "# ==============================================================================\n",
        "# Uninstall previous attempts to ensure a clean slate\n",
        "!pip uninstall -y spacy spacy-transformers transformers tokenizers curated-transformers spacy-curated-transformers curated-tokenizers spacy-lookups-data en-core-web-trf\n",
        "\n",
        "# Install the core libraries - let pip resolve dependencies\n",
        "print(\"Installing spaCy, spacy-transformers, and lookups data...\")\n",
        "!pip install -q spacy spacy-transformers spacy-lookups-data\n",
        "\n",
        "# Download the correct transformer model using the standard command\n",
        "print(\"\\nDownloading the 'en_core_web_trf' model...\")\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "print(\"\\n--- Libraries and Model Installed ---\")\n",
        "print(\"✅ spaCy\")\n",
        "print(\"✅ spacy-transformers (and its dependencies like transformers, tokenizers)\")\n",
        "print(\"✅ spacy-lookups-data\")\n",
        "print(\"✅ en_core_web_trf model\")\n",
        "print(\"\\n--> IMPORTANT: Please RESTART the Colab Runtime now! <--\")\n",
        "print(\"(Go to Runtime > Restart session / Restart runtime)\")"
      ],
      "metadata": {
        "id": "1QU3VMb79ED6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Import All Libraries\n",
        "# ==============================================================================\n",
        "# For M1\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# For M2\n",
        "import spacy\n",
        "from spacy.tokens import DocBin, Doc\n",
        "from spacy.training.example import Example\n",
        "import re\n",
        "import ast  # For safely evaluating the string-list in 'ner_entities'\n",
        "import random\n",
        "from spacy.cli.train import train\n",
        "from spacy.scorer import Scorer\n",
        "import json\n",
        "\n",
        "# For File Upload\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "hRU-A0vh9Kdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Load Data (Robust Version)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"--- Please upload your 'preprocessed_content.csv' file ---\")\n",
        "print(\"(This is the file from inside the .zip)\")\n",
        "\n",
        "try:\n",
        "    uploaded = files.upload()\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(f\"\\nSuccessfully uploaded '{file_name}'\")\n",
        "\n",
        "    print(\"\\nLoading data using the robust 'python' engine...\")\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        io.BytesIO(uploaded[file_name]),\n",
        "        encoding='latin1',\n",
        "        on_bad_lines='skip',\n",
        "        engine='python'\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"\\n!!! ERROR: The loaded DataFrame is empty.\")\n",
        "    else:\n",
        "        print(f\"--- Successfully loaded data into DataFrame ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "id": "OswTWh6M98yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 4: Preprocessing Pipeline (Milestone 1 Cleanup)\n",
        "# ==============================================================================\n",
        "if 'df' in locals() and not df.empty:\n",
        "    print(\"--- Starting preprocessing pipeline ---\")\n",
        "\n",
        "    # Fix Index Column\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        print(\"Found 'Unnamed: 0' column, setting it as the index.\")\n",
        "        df = df.set_index('Unnamed: 0')\n",
        "        df.index.name = None # Clear the index name\n",
        "\n",
        "    print(f\"Original entity count: {len(df)}\")\n",
        "    print(\"\\n--- Data is clean and ready. ---\")\n",
        "    df.info()\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found or is empty.\")"
      ],
      "metadata": {
        "id": "v4I964W8-X6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 5: Milestone 2.1 - Preprocessing Effectiveness & Load Model\n",
        "# ==============================================================================\n",
        "if 'df' in locals():\n",
        "    print(\"--- Demonstrating Preprocessing (Tokenization & Lemmatization) ---\")\n",
        "\n",
        "    try:\n",
        "        # Load the specific transformer model we downloaded\n",
        "        nlp = spacy.load(\"en_core_web_trf\")\n",
        "        print(\"Successfully loaded 'en_core_web_trf' model.\")\n",
        "    except OSError:\n",
        "        print(\"!!! ERROR: 'en_core_web_trf' model not found.\")\n",
        "        print(\"Please ensure Cell 1 ran successfully and you restarted the runtime.\")\n",
        "        nlp = None # Set nlp to None if loading failed\n",
        "\n",
        "    if nlp: # Proceed only if the model loaded\n",
        "        sample_text = str(df['preprocessed_content'].dropna().iloc[0])\n",
        "        sample_text_short = sample_text[:250]\n",
        "\n",
        "        print(f\"\\nProcessing sample text:\\n'{sample_text_short}...'\")\n",
        "        doc = nlp(sample_text_short)\n",
        "\n",
        "        print(\"\\n--- Analysis Results ---\")\n",
        "        print(f\"{'Token':<15} | {'Lemma (Base Form)':<18} | {'Part of Speech':<15}\")\n",
        "        print(\"-\" * 52)\n",
        "        for token in doc:\n",
        "            print(f\"{token.text:<15} | {token.lemma_:<18} | {token.pos_:<15}\")\n",
        "\n",
        "        print(\"\\n--- Preprocessing Demo Complete ---\")\n",
        "\n",
        "else:\n",
        "    print(\"!!! ERROR: Dataframe 'df' not found. Please re-run Cell 3.\")"
      ],
      "metadata": {
        "id": "DuZ1mo5V_z6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "rfJAEQARd_Ox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}